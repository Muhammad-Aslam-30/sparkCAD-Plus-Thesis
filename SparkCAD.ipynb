{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6529b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from copy import copy\n",
    "import re\n",
    "import configparser\n",
    "import queue\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import ast\n",
    "import random\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "\n",
    "\n",
    "class RddAsNode:\n",
    "    \n",
    "    def __init__(self, name, is_cached, number_of_usage, number_of_computations):\n",
    "        self.name = name\n",
    "        self.is_cached = is_cached\n",
    "        self.number_of_usage = number_of_usage\n",
    "        self.number_of_computations = number_of_computations\n",
    "\n",
    "        \n",
    "class Rdd:\n",
    "    \n",
    "    def __init__(self, id, name, parents_lst, stage_id, job_id, is_cached):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.parents_lst = parents_lst\n",
    "        self.stage_id = stage_id\n",
    "        self.job_id = job_id\n",
    "        self.is_cached = is_cached\n",
    "        \n",
    "    def to_json(self):\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"name\": self.name,\n",
    "            \"parents_lst\": self.parents_lst,\n",
    "            \"stage_id\": self.stage_id,\n",
    "            \"job_id\": self.job_id,\n",
    "            \"is_cached\": self.is_cached\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, data):\n",
    "        return cls(\n",
    "            data[\"id\"],\n",
    "            data[\"name\"],\n",
    "            data[\"parents_lst\"],\n",
    "            data[\"stage_id\"],\n",
    "            data[\"job_id\"],\n",
    "            data[\"is_cached\"]\n",
    "        )\n",
    "        \n",
    "class Transformation:\n",
    "    \n",
    "    def __init__(self, from_rdd, to_rdd, is_narrow):\n",
    "        self.from_rdd = from_rdd\n",
    "        self.to_rdd = to_rdd\n",
    "        self.is_narrow = is_narrow\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if (isinstance(other, Transformation)):\n",
    "            return self.from_rdd == other.from_rdd and self.to_rdd == other.to_rdd\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.from_rdd) ^ hash(self.to_rdd)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.from_rdd == other.from_rdd:\n",
    "            self.to_rdd < other.to_rdd\n",
    "        return self.from_rdd < other.from_rdd\n",
    "    \n",
    "class TransformationWithoutI:\n",
    "    \n",
    "    def __init__(self, from_rdd, to_rdd, is_narrow):\n",
    "        self.from_rdd = from_rdd\n",
    "        self.to_rdd = to_rdd\n",
    "        self.is_narrow = is_narrow\n",
    "    \n",
    "class CachingPlanItem:\n",
    "    \n",
    "    def __init__(self, stage_id, job_id, rdd_id, is_cache_item):\n",
    "        self.stage_id = stage_id\n",
    "        self.job_id = job_id\n",
    "        self.rdd_id = rdd_id\n",
    "        self.is_cache_item = is_cache_item\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.job_id == other.job_id:\n",
    "            if self.stage_id == other.stage_id:\n",
    "                if self.is_cache_item == other.is_cache_item:\n",
    "                    return self.rdd_id\n",
    "                return self.is_cache_item\n",
    "            return self.stage_id < other.stage_id\n",
    "        return self.job_id < other.job_id\n",
    "\n",
    "class CachingPlanItemWithStorage:\n",
    "    \n",
    "    def __init__(self, stage_id, job_id, rdd_id, is_cache_item, persist_status):\n",
    "        self.stage_id = stage_id\n",
    "        self.job_id = job_id\n",
    "        self.rdd_id = rdd_id\n",
    "        self.is_cache_item = is_cache_item\n",
    "        self.persist_status = persist_status\n",
    "        \n",
    "class Utility():\n",
    "    def get_absolute_path(path):\n",
    "        if not os.path.isabs(path):\n",
    "            return str(Path().absolute()) + '/' + path\n",
    "        return path\n",
    "\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3    \n",
    "\n",
    "    \n",
    "class FactHub():\n",
    "    \n",
    "    app_name = \"\"\n",
    "    job_info_dect = {}\n",
    "    stage_info_dect = {}\n",
    "    stage_job_dect = {}\n",
    "    stage_name_dect = {}\n",
    "    submitted_stage_last_rdd_dect = {}\n",
    "    stage_no_of_tasks = {}\n",
    "    stage_i_operator_dect = defaultdict(list)\n",
    "    stage_i_operators_id = defaultdict(list)\n",
    "    submitted_stages = set()\n",
    "    rdds_lst = []\n",
    "    operator_partition_size = {}\n",
    "    rddID_in_stage = defaultdict(list)\n",
    "    stage_operator_partition = {}\n",
    "    #total = accumulables_update + bytes_written\n",
    "    stage_total = {}\n",
    "    job_last_rdd = {}\n",
    "    job_last_rdd_dect = {}\n",
    "    job_last_stage = {}\n",
    "    rdd_id_stage_with_max_tasks = {}\n",
    "    task_in_which_stage = {}\n",
    "    stage_has_what_tasks = defaultdict(list)\n",
    "    rdds_lst_index_dict = {}\n",
    "    taskid_launchtime = {}\n",
    "    taskid_finishtime = {}\n",
    "    taskid_last_processing_time = defaultdict(int)\n",
    "    taskid_operator_dect = defaultdict(list)\n",
    "    taskid_bytes_read = {}\n",
    "    \n",
    "    root_rdd_size = {}\n",
    "    rddID_size = defaultdict(int)\n",
    "    last_rdd_size = {}\n",
    "    operator_timestamp = defaultdict(int)\n",
    "    operator_in_which_tasks = {}\n",
    "    rdds_lst_renumbered = []\n",
    "    transformations = []\n",
    "    \n",
    "    rdds_lst_refactored = []\n",
    "    rdds_lst_InstrumentedRdds = []\n",
    "    rdds_lst_InstrumentedRdds_id = []\n",
    "    \n",
    "    stage_shuffle_writetime_dict = {}\n",
    "    stage_shuffle_readtime_dict = {}\n",
    "    stage_shuffle_time_dict = {}\n",
    "    shuffled_rdds_id = []\n",
    "    \n",
    "\n",
    "    def flush():\n",
    "        FactHub.app_name = \"\"\n",
    "        FactHub.job_info_dect = {}\n",
    "        FactHub.stage_info_dect = {}\n",
    "        FactHub.stage_job_dect = {}\n",
    "        FactHub.stage_name_dect = {}\n",
    "        FactHub.submitted_stage_last_rdd_dect = {}\n",
    "        FactHub.stage_no_of_tasks = {}\n",
    "        FactHub.stage_i_operator_dect = defaultdict(list)\n",
    "        FactHub.stage_i_operators_id = defaultdict(list)\n",
    "        FactHub.submitted_stages.clear()\n",
    "        FactHub.rdds_lst = []\n",
    "        FactHub.operator_partition_size = {}\n",
    "        FactHub.rddID_in_stage = defaultdict(list)\n",
    "        FactHub.stage_operator_partition = {}\n",
    "        #total = accumulables_update + bytes_written\n",
    "        FactHub.stage_total = {}\n",
    "        FactHub.job_last_rdd = {}\n",
    "        FactHub.job_last_rdd_dect = {}\n",
    "        FactHub.job_last_stage = {}\n",
    "        FactHub.rdd_id_stage_with_max_tasks = {}\n",
    "        FactHub.task_in_which_stage = {}\n",
    "        FactHub.stage_has_what_tasks = defaultdict(list)\n",
    "        FactHub.rdds_lst_index_dict = {}\n",
    "        FactHub.taskid_launchtime = {}\n",
    "        FactHub.taskid_finishtime = {}\n",
    "        FactHub.taskid_last_processing_time = defaultdict(int)\n",
    "        FactHub.taskid_operator_dect = defaultdict(list)\n",
    "        FactHub.taskid_bytes_written = {}\n",
    "        FactHub.root_rdd_size = {}\n",
    "        FactHub.rddID_size = defaultdict(int)\n",
    "        FactHub.last_rdd_size = {}\n",
    "        FactHub.operator_timestamp = defaultdict(int)\n",
    "        FactHub.operator_in_which_tasks = {}\n",
    "        FactHub.rdds_lst_renumbered = []\n",
    "        FactHub.transformations = []\n",
    "        FactHub.rdds_lst_refactored = []\n",
    "        FactHub.rdds_lst_InstrumentedRdds = []\n",
    "        FactHub.rdds_lst_InstrumentedRdds_id = []\n",
    "        FactHub.stage_shuffle_writetime_dict = {}\n",
    "        FactHub.stage_shuffle_readtime_dict = {}\n",
    "        FactHub.stage_shuffle_time_dict = {}\n",
    "        FactHub.shuffled_rdds_id = []\n",
    "        \n",
    "class AnalysisHub():\n",
    "    \n",
    "    transformations_set = set()\n",
    "    rdd_num_of_computations = defaultdict(int)\n",
    "    rdd_num_of_usage = defaultdict(int)\n",
    "    anomalies_dict = {}\n",
    "    stage_computed_rdds = {}\n",
    "    stage_used_rdds = {}\n",
    "    computed_rdds = set()\n",
    "    rdd_usage_lifetime_dict = {}\n",
    "    caching_plan_lst = []\n",
    "    memory_footprint_lst = []\n",
    "    cached_rdds_set = set()\n",
    "    non_cached_rdds_set = set()\n",
    "    cached_rdds_lst = []\n",
    "    rdds_computation_time = {}\n",
    "    recommended_schedule_cache_at = {}\n",
    "    recommended_schedule_unpersist_after = {}\n",
    "    transformation_without_i = []\n",
    "    transformation_from_to = {}\n",
    "    transformation_to_from = defaultdict(list)\n",
    "    memory_size = 0\n",
    "    memory_read_capacity = 0\n",
    "    memory_write_capacity = 0\n",
    "    disk_size = 0\n",
    "    disk_read_capacity = 0\n",
    "    disk_write_capacity = 0\n",
    "    time_model_scenario1 = defaultdict(int)\n",
    "    time_model_scenario2 = defaultdict(int)\n",
    "    time_model_scenario3 = defaultdict(int)\n",
    "    rdd_cache_order = []\n",
    "    memory_management_list = []\n",
    "    disk_management_list = []\n",
    "    persist_status_recorder = {}\n",
    "    \n",
    "    def flush():\n",
    "        AnalysisHub.transformations_set.clear()\n",
    "        AnalysisHub.rdd_num_of_computations = defaultdict(int)\n",
    "        AnalysisHub.rdd_num_of_usage = defaultdict(int)\n",
    "        AnalysisHub.anomalies_dict = {}\n",
    "        AnalysisHub.stage_computed_rdds = {}\n",
    "        AnalysisHub.stage_used_rdds = {}\n",
    "        AnalysisHub.computed_rdds.clear()\n",
    "        AnalysisHub.rdd_usage_lifetime_dict = {}\n",
    "        AnalysisHub.caching_plan_lst = []\n",
    "        #AnalysisHub.memory_footprint_lst = []\n",
    "        #AnalysisHub.cached_rdds_set.clear()\n",
    "        #AnalysisHub.non_cached_rdds_set.clear()\n",
    "        AnalysisHub.cached_rdds_lst = []\n",
    "        AnalysisHub.rdds_computation_time = {}\n",
    "        AnalysisHub.recommended_schedule_cache_at = {}\n",
    "        AnalysisHub.recommended_schedule_unpersist_after = {}\n",
    "        AnalysisHub.transformation_without_i = []\n",
    "        AnalysisHub.transformation_from_to = {}\n",
    "        AnalysisHub.transformation_to_from = defaultdict(list)\n",
    "        AnalysisHub.memory_size = 0\n",
    "        AnalysisHub.memory_read_capacity = 0\n",
    "        AnalysisHub.memory_write_capacity = 0\n",
    "        AnalysisHub.disk_size = 0\n",
    "        AnalysisHub.disk_read_capacity = 0\n",
    "        AnalysisHub.disk_write_capacity = 0\n",
    "        AnalysisHub.time_model_scenario1 = defaultdict(int)\n",
    "        AnalysisHub.time_model_scenario2 = defaultdict(int)\n",
    "        AnalysisHub.time_model_scenario3 = defaultdict(int)\n",
    "        AnalysisHub.rdd_cache_order = []\n",
    "        AnalysisHub.memory_management_list = []\n",
    "        AnalysisHub.disk_management_list = []\n",
    "        AnalysisHub.persist_status_recorder = {}\n",
    "        \n",
    "class Parser():    \n",
    "    \n",
    "    def prepare(raw_log_file):\n",
    "        all_events_lst = pd.read_json(raw_log_file, lines=True)\n",
    "        FactHub.app_name = all_events_lst[all_events_lst['Event'] == 'SparkListenerApplicationStart']['App Name'].tolist()[0]\n",
    "        Parser.prepare_from_stage_submitted_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerStageSubmitted'])\n",
    "        Parser.prepare_from_job_start_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerJobStart'])\n",
    "        Parser.prepare_from_task_end_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerTaskEnd'])\n",
    "        Parser.prepare_RDD_ID_from_stage_submitted_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerStageSubmitted'])\n",
    "        Parser.prepare_root_from_stage_completed_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerStageCompleted'])\n",
    "        Parser.prepare_leaf_from_task_end_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerTaskEnd'])\n",
    "        Parser.prepare_from_task_end_events_for_timestamp(all_events_lst[all_events_lst['Event'] == 'SparkListenerTaskEnd'])\n",
    "    \n",
    "    def prepare_from_task_end_events_for_timestamp(task_end_events):\n",
    "        rdd = 0\n",
    "        for key, value in FactHub.taskid_operator_dect.items():\n",
    "            for item in value:\n",
    "                operator_id = item['Operator ID']\n",
    "                if operator_id in FactHub.operator_in_which_tasks:\n",
    "                    FactHub.operator_in_which_tasks[operator_id].append(key)\n",
    "                else:\n",
    "                    FactHub.operator_in_which_tasks[operator_id] = [key]       \n",
    "        for task_id in FactHub.taskid_operator_dect.keys():\n",
    "            #To get stage_id for this task\n",
    "            stage_id = FactHub.task_in_which_stage[task_id]\n",
    "            for j in FactHub.job_last_stage.keys():\n",
    "                if (FactHub.job_last_stage[j] == stage_id):\n",
    "                    rdd = FactHub.job_last_rdd[j]\n",
    "            total_entries = 0\n",
    "            # To calculate total operators count in one task\n",
    "            for x in FactHub.taskid_operator_dect[task_id]:\n",
    "                total_entries = total_entries + 1\n",
    "            for i in range(total_entries):\n",
    "                if (i == 0):\n",
    "                    first_operator_time = FactHub.taskid_operator_dect[task_id][i]['Timestamp'] - FactHub.taskid_launchtime[task_id]\n",
    "                    first_operator_time = abs(first_operator_time)\n",
    "                    #print(\"first operator time: \" + str(first_operator_time))\n",
    "                    if FactHub.taskid_operator_dect[task_id][i]['Operator ID'] in FactHub.operator_timestamp.keys():\n",
    "                        FactHub.operator_timestamp[FactHub.taskid_operator_dect[task_id][i]['Operator ID']] = FactHub.operator_timestamp[FactHub.taskid_operator_dect[task_id][i]['Operator ID']] + first_operator_time\n",
    "                    else:\n",
    "                        FactHub.operator_timestamp[FactHub.taskid_operator_dect[task_id][i]['Operator ID']] = first_operator_time\n",
    "                #elif i!=0 and i!=total_entries-1:\n",
    "                else:\n",
    "                    middle_operator_time = FactHub.taskid_operator_dect[task_id][i]['Timestamp'] - FactHub.taskid_operator_dect[task_id][i-1]['Timestamp']\n",
    "                    middle_operator_time = abs(middle_operator_time)\n",
    "                    #print(\"middle operator time: \" + str(middle_operator_time))\n",
    "                    if FactHub.taskid_operator_dect[task_id][i]['Operator ID'] in FactHub.operator_timestamp.keys():\n",
    "                        FactHub.operator_timestamp[FactHub.taskid_operator_dect[task_id][i]['Operator ID']] = FactHub.operator_timestamp[FactHub.taskid_operator_dect[task_id][i]['Operator ID']] + middle_operator_time\n",
    "                    else:\n",
    "                        FactHub.operator_timestamp[FactHub.taskid_operator_dect[task_id][i]['Operator ID']] = middle_operator_time\n",
    "                #To calculate last remaining time between task finish time and last processed operation in a task and save it in a dict for \n",
    "                #caculating the time of last rdd in a job\n",
    "                if (i == total_entries-1):\n",
    "                    remaining_time = FactHub.taskid_finishtime[task_id] - FactHub.taskid_operator_dect[task_id][i]['Timestamp']\n",
    "                    remaining_time = abs(remaining_time)\n",
    "                    FactHub.taskid_last_processing_time[task_id] = remaining_time\n",
    "\n",
    "        #To calculate timestamp of last rdd of a job (logic = used timestamp of an operator before the last rdd and task's finish time)\n",
    "        for job in FactHub.job_last_rdd:\n",
    "            last_rdd_id = FactHub.job_last_rdd[job]\n",
    "            if last_rdd_id not in FactHub.operator_timestamp:\n",
    "                stages_of_rdd = FactHub.rddID_in_stage[last_rdd_id]\n",
    "                for stage in stages_of_rdd:\n",
    "                    tasks_list = FactHub.stage_has_what_tasks[stage]\n",
    "                    for task in tasks_list:\n",
    "                        FactHub.operator_timestamp[last_rdd_id] = FactHub.operator_timestamp[last_rdd_id] + FactHub.taskid_last_processing_time[task]\n",
    "        \n",
    "    def prepare_leaf_from_task_end_events(task_end_events):\n",
    "        stage_id_for_a_task = task_end_events['Stage ID'].tolist()\n",
    "        #dictionary for task and the stage it belongs to\n",
    "        for i, stage in enumerate(stage_id_for_a_task):\n",
    "            FactHub.task_in_which_stage[i] = int(stage)\n",
    "        for task in FactHub.task_in_which_stage:\n",
    "            if FactHub.task_in_which_stage[task] in FactHub.stage_has_what_tasks:\n",
    "                FactHub.stage_has_what_tasks[FactHub.task_in_which_stage[task]].append(task)\n",
    "            else:\n",
    "                FactHub.stage_has_what_tasks[FactHub.task_in_which_stage[task]].append(task)\n",
    "        temp_dict = {}\n",
    "        queue = []\n",
    "        accumulables_info_list = task_end_events['Task Info'].tolist()\n",
    "        for i, task_end in enumerate(task_end_events['Task Info'].tolist()):\n",
    "            FactHub.taskid_launchtime[task_end['Task ID']] = task_end['Launch Time']\n",
    "            FactHub.taskid_finishtime[task_end['Task ID']] = task_end['Finish Time']\n",
    "            accumulables_update = 0\n",
    "            total = 0\n",
    "            for j, accumulables in enumerate(task_end['Accumulables']):\n",
    "                if accumulables['Name'] == \"internal.metrics.resultSize\":\n",
    "                    accumulables_update = accumulables['Update']\n",
    "            for k, task_metrics in enumerate(task_end_events['Task Metrics']):\n",
    "                    if \"Bytes Written\" in task_metrics['Output Metrics'].keys():\n",
    "                        bytes_written = task_metrics['Output Metrics']['Bytes Written']\n",
    "            total = accumulables_update + bytes_written\n",
    "            queue.append(total)\n",
    "        for i in FactHub.stage_no_of_tasks:\n",
    "            total = 0\n",
    "            for x in range(0, FactHub.stage_no_of_tasks[i]):\n",
    "                if len(queue)==0:\n",
    "                    break\n",
    "                total = total + queue[0]\n",
    "                queue.pop(0)\n",
    "            FactHub.stage_total[i] = total\n",
    "        for job_id in FactHub.job_last_stage:\n",
    "            last_stage = FactHub.job_last_stage[job_id]\n",
    "            if last_stage in FactHub.stage_total.keys():\n",
    "                total = FactHub.stage_total[last_stage]\n",
    "                FactHub.last_rdd_size[FactHub.job_last_rdd[job_id]] = total\n",
    "                FactHub.rddID_size[FactHub.job_last_rdd[job_id]] = total\n",
    "        queue = []\n",
    "        for i, task_metrics in enumerate(task_end_events['Task Metrics']):\n",
    "            if \"Shuffle Write Time\" in task_metrics['Shuffle Write Metrics'].keys():\n",
    "                shuffle_write_time = task_metrics['Shuffle Write Metrics']['Shuffle Write Time']\n",
    "                # Converting default wait time from nanoseconds to milliseconds\n",
    "                shuffle_write_time = shuffle_write_time / 1000000\n",
    "                shuffle_write_time = round(shuffle_write_time, 1)\n",
    "                queue.append(shuffle_write_time)\n",
    "        for i, stage in enumerate(stage_id_for_a_task):\n",
    "            if int(stage) not in FactHub.stage_shuffle_writetime_dict.keys():\n",
    "                FactHub.stage_shuffle_writetime_dict[int(stage)] = queue[0]\n",
    "                queue.pop(0)\n",
    "            else:\n",
    "                FactHub.stage_shuffle_writetime_dict[int(stage)] = queue[0] + FactHub.stage_shuffle_writetime_dict[int(stage)]\n",
    "                queue.pop(0)\n",
    "        queue1 = []\n",
    "        for i, task_metrics in enumerate(task_end_events['Task Metrics']):\n",
    "            if \"Fetch Wait Time\" in task_metrics['Shuffle Read Metrics'].keys():\n",
    "                shuffle_read_time = task_metrics['Shuffle Read Metrics']['Fetch Wait Time']\n",
    "                queue1.append(shuffle_read_time)\n",
    "        for i, stage in enumerate(stage_id_for_a_task):\n",
    "            if int(stage) not in FactHub.stage_shuffle_readtime_dict.keys():\n",
    "                FactHub.stage_shuffle_readtime_dict[int(stage)] = queue1[0]\n",
    "                queue1.pop(0)\n",
    "            else:\n",
    "                FactHub.stage_shuffle_readtime_dict[int(stage)] = queue1[0] + FactHub.stage_shuffle_readtime_dict[int(stage)]\n",
    "                queue1.pop(0)\n",
    "        tdict = {}\n",
    "        for i, stage in enumerate(FactHub.submitted_stages):\n",
    "            tdict[i] = stage\n",
    "        for i, stage in enumerate(FactHub.submitted_stages):\n",
    "            if stage == 0:\n",
    "                FactHub.stage_shuffle_time_dict[stage] = round(FactHub.stage_shuffle_readtime_dict[stage], 1)\n",
    "            else:\n",
    "                FactHub.stage_shuffle_time_dict[stage] = round((FactHub.stage_shuffle_readtime_dict[stage] + FactHub.stage_shuffle_writetime_dict[tdict[i-1]]), 1)\n",
    "        #prepare a dict with task id and its shuffle bytes written\n",
    "        taskid_queue = []\n",
    "        for i, task_end in enumerate(task_end_events['Task Info'].tolist()):\n",
    "            taskid_queue.append(task_end['Task ID'])\n",
    "        for i, task_metric in enumerate(task_end_events['Task Metrics']):\n",
    "            if \"Shuffle Bytes Written\" in task_metric['Shuffle Write Metrics'].keys():\n",
    "                bytes_written = task_metric['Shuffle Write Metrics']['Shuffle Bytes Written']\n",
    "                if bytes_written == 0:\n",
    "                    if \"Bytes Read\" in task_metric['Input Metrics'].keys():\n",
    "                        bytes_read = task_metric['Input Metrics']['Bytes Read']\n",
    "                        FactHub.taskid_bytes_written[taskid_queue[0]] = bytes_read\n",
    "                else:\n",
    "                    FactHub.taskid_bytes_written[taskid_queue[0]] = bytes_written\n",
    "            taskid_queue.pop(0)\n",
    "        \n",
    "    def prepare_root_from_stage_completed_events(stage_submitted_events):\n",
    "        max_size_root_rdd = 0\n",
    "        for i, submitted_stage in enumerate(stage_submitted_events['Stage Info'].tolist()):\n",
    "            stage_id = submitted_stage['Stage ID']\n",
    "            for j, rdd_info in enumerate(submitted_stage['RDD Info']):\n",
    "                parent_id = rdd_info['Parent IDs']\n",
    "                if not parent_id:\n",
    "                    root_rdd = rdd_info['RDD ID']\n",
    "            for k, read_bytes in enumerate(submitted_stage['Accumulables']):\n",
    "                if read_bytes['Name'] == 'internal.metrics.input.bytesRead' and read_bytes['Value'] > max_size_root_rdd:\n",
    "                    max_size_root_rdd = read_bytes['Value']\n",
    "                    root_rdd = rdd_info['RDD ID']\n",
    "        FactHub.root_rdd_size[root_rdd] = max_size_root_rdd\n",
    "        FactHub.rddID_size[root_rdd] = max_size_root_rdd       \n",
    "        \n",
    "    def prepare_RDD_ID_from_stage_submitted_events(stage_submitted_events):\n",
    "        for i, submitted_stage in enumerate(stage_submitted_events['Stage Info'].tolist()):\n",
    "            stage_id = submitted_stage['Stage ID']\n",
    "            for j, rdd_info in enumerate(submitted_stage['RDD Info']):\n",
    "                rdd_id = rdd_info['RDD ID']\n",
    "                FactHub.rddID_in_stage[rdd_id].append(stage_id)\n",
    "        # To order the FactHub.rddID_in_stage in ascending order based on key\n",
    "        dict1 = OrderedDict(sorted(FactHub.rddID_in_stage.items()))\n",
    "        for i in dict1:\n",
    "            max_no_of_tasks = 0\n",
    "            for j in dict1[i]:\n",
    "                if FactHub.stage_no_of_tasks[j] > max_no_of_tasks:\n",
    "                    max_no_of_tasks = FactHub.stage_no_of_tasks[j]\n",
    "                    respective_stage = j\n",
    "            FactHub.rdd_id_stage_with_max_tasks[i] = respective_stage\n",
    "        for operator in FactHub.operator_partition_size:\n",
    "            if operator in FactHub.rdd_id_stage_with_max_tasks.keys():\n",
    "                stage_with_max_tasks = FactHub.rdd_id_stage_with_max_tasks[operator]\n",
    "            for stage in FactHub.stage_i_operators_id:\n",
    "                if operator in FactHub.stage_i_operators_id[stage]:\n",
    "                    operator_s_stage = stage\n",
    "            if operator_s_stage == stage_with_max_tasks:\n",
    "                FactHub.rddID_size[operator] = FactHub.operator_partition_size[operator]\n",
    "            else:\n",
    "                FactHub.rddID_size[operator] = FactHub.operator_partition_size[operator] * (FactHub.stage_no_of_tasks[stage_with_max_tasks])\n",
    "            \n",
    "    def prepare_from_job_start_events(job_start_events):\n",
    "        job_ids_list = job_start_events['Job ID'].tolist()\n",
    "        job_stage_info_list = job_start_events['Stage Infos'].tolist()\n",
    "        for job_num, job_rec in enumerate(job_stage_info_list):\n",
    "            job_id = int(job_ids_list[job_num])\n",
    "            FactHub.job_info_dect[job_id] = job_rec\n",
    "            id_of_last_rdd_in_job = -1\n",
    "            for stage_num, stage_rec in enumerate(job_rec):\n",
    "                stage_id = int(stage_rec['Stage ID'])\n",
    "                FactHub.stage_job_dect[stage_id] = job_id\n",
    "                FactHub.stage_info_dect[stage_id] = stage_rec\n",
    "                FactHub.stage_name_dect[stage_id] = stage_rec['Stage Name']\n",
    "                # To print the number of tasks in each stage\n",
    "                FactHub.stage_no_of_tasks[stage_id] = stage_rec['Number of Tasks']\n",
    "                id_of_last_rdd_in_stage = -1\n",
    "                for stage_rdd_num, stage_rdd_rec in enumerate(stage_rec['RDD Info']):\n",
    "                    rdd_id = stage_rdd_rec['RDD ID']\n",
    "                    is_cached = stage_rdd_rec['Storage Level']['Use Memory'] or stage_rdd_rec['Storage Level']['Use Disk']\n",
    "                    FactHub.rdds_lst.append(Rdd(rdd_id, stage_rdd_rec['Name'] + '\\n' + stage_rdd_rec['Callsite'], stage_rdd_rec['Parent IDs'], stage_id, job_id, is_cached))\n",
    "                    if id_of_last_rdd_in_job < rdd_id:\n",
    "                        id_of_last_rdd_in_job = rdd_id\n",
    "                    if id_of_last_rdd_in_stage < rdd_id:\n",
    "                        id_of_last_rdd_in_stage = rdd_id\n",
    "                if stage_id in FactHub.submitted_stages:\n",
    "                    FactHub.submitted_stage_last_rdd_dect[stage_id] = id_of_last_rdd_in_stage\n",
    "            #Dictionary with job and last rdd along with stage name\n",
    "            FactHub.job_last_rdd_dect[job_id] = (id_of_last_rdd_in_job, stage_rec['Stage Name'])\n",
    "            #Dictionary with job and last rdd details without stage name\n",
    "            FactHub.job_last_rdd[job_id] = (id_of_last_rdd_in_job)\n",
    "        for i in FactHub.job_last_rdd_dect:\n",
    "            for j in FactHub.stage_job_dect:\n",
    "                if (FactHub.stage_job_dect[j] == i):\n",
    "                    FactHub.job_last_stage[i] = j            \n",
    "        \n",
    "    def prepare_from_task_end_events(task_end_events):\n",
    "        task_stage_ids_list = task_end_events['Stage ID'].tolist()\n",
    "        task_operators_info_list = task_end_events['SparkIOperatorsDetails'].tolist()\n",
    "        for index, task_operator_rec in enumerate(task_operators_info_list):\n",
    "            task_stage_id = int(task_stage_ids_list[index])\n",
    "            FactHub.stage_i_operator_dect[task_stage_id].append(task_operator_rec)  \n",
    "        a = []\n",
    "        operator_ids_list = []\n",
    "        count = 0\n",
    "        for task_stage_i in FactHub.stage_i_operator_dect.keys():\n",
    "            length = len(FactHub.stage_i_operator_dect[task_stage_i])\n",
    "            for lent in range(length):\n",
    "                # To Print lists of operators details\n",
    "                a = FactHub.stage_i_operator_dect[task_stage_i][lent]\n",
    "                FactHub.taskid_operator_dect[count] = a\n",
    "                count = count + 1\n",
    "                for a_seperate_list in a:\n",
    "                    if a_seperate_list['Operator ID'] in FactHub.operator_partition_size.keys():\n",
    "                        temp_dict_value = FactHub.operator_partition_size[a_seperate_list['Operator ID']]\n",
    "                        FactHub.operator_partition_size.update({a_seperate_list['Operator ID'] : (a_seperate_list['Partition Size'] + temp_dict_value)})\n",
    "                    else:\n",
    "                        FactHub.operator_partition_size[a_seperate_list['Operator ID']] = a_seperate_list['Partition Size']\n",
    "                        FactHub.stage_i_operators_id[task_stage_i].append(a_seperate_list['Operator ID'])\n",
    "                FactHub.stage_operator_partition[task_stage_i] = FactHub.operator_partition_size\n",
    "        \n",
    "    def prepare_from_stage_submitted_events(stage_submitted_events):\n",
    "        for index, submitted_stage in enumerate(stage_submitted_events['Stage Info'].tolist()):\n",
    "            FactHub.submitted_stages.add(submitted_stage['Stage ID'])\n",
    "    \n",
    "class Analyzer():\n",
    "\n",
    "    def is_narrow_transformation(rdd_id, parent_id):\n",
    "        rdd_stages_set = set()\n",
    "        parent_stages_set = set()\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.id == rdd_id:\n",
    "                rdd_stages_set.add(rdd.stage_id)\n",
    "            elif rdd.id == parent_id:\n",
    "                parent_stages_set.add(rdd.stage_id)\n",
    "        return len(Utility.intersection(rdd_stages_set, parent_stages_set)) != 0\n",
    "\n",
    "    def prepare_transformations_lst():\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            for parent_id in rdd.parents_lst:\n",
    "                AnalysisHub.transformations_set.add(Transformation(rdd.id, parent_id, Analyzer.is_narrow_transformation(rdd.id, parent_id)))\n",
    "\n",
    "    def add_rdd_and_its_parents_if_it_is_computed_in_stage(rdd_id, stage_id):#recursive\n",
    "        if rdd_id not in AnalysisHub.stage_used_rdds[stage_id]:\n",
    "            AnalysisHub.rdd_num_of_usage[rdd_id] += 1\n",
    "            AnalysisHub.stage_used_rdds[stage_id].add(rdd_id)            \n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.id == rdd_id: \n",
    "                if rdd.is_cached:\n",
    "                    if rdd_id not in AnalysisHub.rdd_usage_lifetime_dict:\n",
    "                        AnalysisHub.rdd_usage_lifetime_dict[rdd.id] = (rdd.stage_id, rdd.job_id, rdd.stage_id, rdd.job_id)\n",
    "                    if AnalysisHub.rdd_usage_lifetime_dict[rdd_id][0] > stage_id:\n",
    "                        AnalysisHub.rdd_usage_lifetime_dict[rdd.id] = (rdd.stage_id, rdd.job_id, AnalysisHub.rdd_usage_lifetime_dict[rdd_id][2], AnalysisHub.rdd_usage_lifetime_dict[rdd_id][3])\n",
    "                    if AnalysisHub.rdd_usage_lifetime_dict[rdd_id][2] < stage_id:\n",
    "                        AnalysisHub.rdd_usage_lifetime_dict[rdd.id] = (AnalysisHub.rdd_usage_lifetime_dict[rdd_id][0], AnalysisHub.rdd_usage_lifetime_dict[rdd_id][1], rdd.stage_id, rdd.job_id)\n",
    "            if rdd.id == rdd_id: \n",
    "                if rdd.stage_id == stage_id:\n",
    "                    if rdd.is_cached:\n",
    "                        if rdd_id in AnalysisHub.computed_rdds: #already cached\n",
    "                            return\n",
    "                        AnalysisHub.computed_rdds.add(rdd_id) #cached for the first time\n",
    "                        AnalysisHub.stage_computed_rdds[stage_id].add(rdd_id)\n",
    "                    else:\n",
    "                        if rdd_id in AnalysisHub.computed_rdds: #handeling unpersistance\n",
    "                            AnalysisHub.computed_rdds.remove(rdd_id)\n",
    "                        AnalysisHub.stage_computed_rdds[stage_id].add(rdd_id)\n",
    "                    for parent_id in rdd.parents_lst:\n",
    "                        if Analyzer.is_narrow_transformation(rdd.id, parent_id):\n",
    "                            Analyzer.add_rdd_and_its_parents_if_it_is_computed_in_stage(parent_id, stage_id)\n",
    "\n",
    "    def calc_num_of_computations_of_rdds():\n",
    "        AnalysisHub.rdd_usage_lifetime_dict = {}\n",
    "        for stage_id in sorted(FactHub.submitted_stage_last_rdd_dect):\n",
    "            id_of_last_rdd_in_stage = FactHub.submitted_stage_last_rdd_dect[stage_id]\n",
    "            AnalysisHub.stage_computed_rdds[stage_id] = set()\n",
    "            AnalysisHub.stage_used_rdds[stage_id] = set()\n",
    "            Analyzer.add_rdd_and_its_parents_if_it_is_computed_in_stage(id_of_last_rdd_in_stage, stage_id)            \n",
    "        for stage_id in AnalysisHub.stage_computed_rdds:\n",
    "            for rdd_id in AnalysisHub.stage_computed_rdds[stage_id]:\n",
    "                AnalysisHub.rdd_num_of_computations[rdd_id] += 1\n",
    "\n",
    "    def prepare_anomalies_dict():\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            rdd.name, rdd.is_cached, AnalysisHub.rdd_num_of_usage[rdd.id], AnalysisHub.rdd_num_of_computations[rdd.id]\n",
    "            if rdd.is_cached and AnalysisHub.rdd_num_of_usage[rdd.id] <= int(config['Caching_Anomalies']['rdds_computation_tolerance_threshold']):\n",
    "                AnalysisHub.anomalies_dict[rdd.id] = \"unneeded cache\"\n",
    "            elif not rdd.is_cached and AnalysisHub.rdd_num_of_computations[rdd.id] > int(config['Caching_Anomalies']['rdds_computation_tolerance_threshold']):\n",
    "                AnalysisHub.anomalies_dict[rdd.id] = \"recomputation\"\n",
    "\n",
    "    def prepare_caching_plan():\n",
    "        AnalysisHub.caching_plan_lst = []\n",
    "        for rdd_id, rdd_usage_lifetime in AnalysisHub.rdd_usage_lifetime_dict.items():\n",
    "            if config['Caching_Anomalies']['include_caching_anomalies_in_caching_plan'] == \"true\" or rdd_id not in AnalysisHub.anomalies_dict:\n",
    "                AnalysisHub.caching_plan_lst.append(CachingPlanItem(rdd_usage_lifetime[0], rdd_usage_lifetime[1], rdd_id, True))\n",
    "                AnalysisHub.caching_plan_lst.append(CachingPlanItem(rdd_usage_lifetime[2], rdd_usage_lifetime[3], rdd_id, False)) \n",
    "        AnalysisHub.memory_footprint_lst = []\n",
    "        incremental_rdds_set = set()\n",
    "        for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "            if caching_plan_item.is_cache_item:\n",
    "                incremental_rdds_set.add(caching_plan_item.rdd_id)\n",
    "            else:\n",
    "                incremental_rdds_set.remove(caching_plan_item.rdd_id)\n",
    "            AnalysisHub.memory_footprint_lst.append((caching_plan_item.job_id, caching_plan_item.stage_id, (incremental_rdds_set.copy())))\n",
    "            \n",
    "    def analyze_caching_anomalies():\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.id in AnalysisHub.cached_rdds_set:\n",
    "                rdd.is_cached = True\n",
    "            if rdd.id in AnalysisHub.non_cached_rdds_set:\n",
    "                rdd.is_cached = False\n",
    "        Analyzer.calc_num_of_computations_of_rdds()\n",
    "        Analyzer.prepare_anomalies_dict() \n",
    "        Analyzer.prepare_caching_plan() \n",
    "\n",
    "class SparkDataflowVisualizer():\n",
    "    def init():\n",
    "        #AnalysisHub.cached_rdds_set.clear()\n",
    "        #AnalysisHub.non_cached_rdds_set.clear()\n",
    "        FactHub.flush()\n",
    "        #AnalysisHub.flush()\n",
    "    \n",
    "    def parse(raw_log_file):\n",
    "        Parser.prepare(raw_log_file)\n",
    "        \n",
    "    def analyze():\n",
    "        #AnalysisHub.flush()\n",
    "        Analyzer.prepare_transformations_lst()\n",
    "        Analyzer.analyze_caching_anomalies()\n",
    "\n",
    "    def rdds_lst_refactor():\n",
    "        for i, rdd in enumerate(FactHub.rdds_lst):\n",
    "            if (\"InstrumentedRDD\" not in rdd.name):\n",
    "                FactHub.rdds_lst_refactored.append(rdd)\n",
    "            else:\n",
    "                FactHub.rdds_lst_InstrumentedRdds_id.append(rdd.id)\n",
    "                FactHub.rdds_lst_InstrumentedRdds.append(rdd)\n",
    "        # Temporary list 't' will store all the cached rdds ids\n",
    "        t = []\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.is_cached:\n",
    "                t.append(rdd.id)\n",
    "        t = list(dict.fromkeys(t))\n",
    "        # Temporary list 't1' will store the rdds all details with updated cache status\n",
    "        t1 = []\n",
    "        for rdd in FactHub.rdds_lst_refactored:\n",
    "            #print(\"rdd id: \" + str(rdd.id) + \" cache: \" + str(rdd.is_cached))\n",
    "            if rdd.id+1 in t:\n",
    "                t1.append(Rdd(rdd.id, rdd.name, rdd.parents_lst, rdd.stage_id, rdd.job_id, True))\n",
    "            else:\n",
    "                t1.append(Rdd(rdd.id, rdd.name, rdd.parents_lst, rdd.stage_id, rdd.job_id, False))\n",
    "        # Flushing all the details in the 'FactHub.rdds_lst_refactored' list and dump it again with all the details from 't1'\n",
    "        # in order to update the caching staus in the 'FactHub.rdds_lst_refactored' list and to show it in the DAG\n",
    "        FactHub.rdds_lst_refactored = []\n",
    "        for rdd in t1:\n",
    "            FactHub.rdds_lst_refactored.append(rdd)\n",
    "            \n",
    "        temp = []\n",
    "        for rdd in FactHub.rdds_lst_refactored:\n",
    "            temp.append(rdd.id)\n",
    "        temp = list(dict.fromkeys(sorted(temp)))\n",
    "        temp1 = []\n",
    "        temp1 = [temp.index(x) for x in temp]\n",
    "        for x in temp:\n",
    "            FactHub.rdds_lst_index_dict[x] = temp.index(x)\n",
    "        for rdd in FactHub.rdds_lst_refactored:\n",
    "            FactHub.rdds_lst_renumbered.append(Rdd(FactHub.rdds_lst_index_dict[rdd.id],rdd.name, rdd.parents_lst, rdd.stage_id, rdd.job_id, rdd.is_cached))\n",
    "        \n",
    "        # AnalysisHub.transformation_from_to has been used in at config['Drawing']['show_rdd_computation_time'], we need that dict in beforehand \n",
    "        # thats why the below loops have been implemented before \"dot = grahpviz.Digraph\" code line\n",
    "        # to create AnalysisHub.transformation_without_i to contain rdd ids without instrumentation ids but will have duplicates\n",
    "        for transformation in sorted(AnalysisHub.transformations_set):\n",
    "            if transformation.to_rdd not in FactHub.rdds_lst_InstrumentedRdds_id and transformation.from_rdd not in FactHub.rdds_lst_InstrumentedRdds_id:\n",
    "                AnalysisHub.transformation_without_i.append(TransformationWithoutI(transformation.from_rdd, transformation.to_rdd, Analyzer.is_narrow_transformation(transformation.from_rdd, transformation.to_rdd)))\n",
    "            if transformation.from_rdd in FactHub.rdds_lst_InstrumentedRdds_id:\n",
    "                AnalysisHub.transformation_without_i.append(TransformationWithoutI(transformation.from_rdd - 1, transformation.to_rdd, Analyzer.is_narrow_transformation(transformation.from_rdd, transformation.to_rdd)))\n",
    "            if transformation.to_rdd in FactHub.rdds_lst_InstrumentedRdds_id:\n",
    "                AnalysisHub.transformation_without_i.append(TransformationWithoutI(transformation.from_rdd, transformation.to_rdd - 1, Analyzer.is_narrow_transformation(transformation.from_rdd, transformation.to_rdd)))\n",
    "        # To refactor AnalysisHub.transformation_without_i to remove duplicates\n",
    "        temp_lst_for_transformation_without_i = []\n",
    "        for transformation in AnalysisHub.transformation_without_i:\n",
    "            if(transformation.to_rdd != transformation.from_rdd):\n",
    "                temp_lst_for_transformation_without_i.append(transformation)\n",
    "        AnalysisHub.transformation_without_i.clear()\n",
    "        for transformation in temp_lst_for_transformation_without_i:\n",
    "            AnalysisHub.transformation_without_i.append(transformation)\n",
    "        for transformation in AnalysisHub.transformation_without_i:\n",
    "            AnalysisHub.transformation_from_to[transformation.from_rdd] = transformation.to_rdd\n",
    "        for transformation in AnalysisHub.transformation_without_i:\n",
    "                AnalysisHub.transformation_to_from[transformation.to_rdd].append(transformation.from_rdd)\n",
    "            \n",
    "        # Recorrecting RDD's number of usages\n",
    "        AnalysisHub.rdd_num_of_usage = defaultdict(int)\n",
    "        for transformation in AnalysisHub.transformation_without_i:\n",
    "            if transformation.to_rdd in AnalysisHub.rdd_num_of_usage.keys(): \n",
    "                AnalysisHub.rdd_num_of_usage[transformation.to_rdd] = AnalysisHub.rdd_num_of_usage[transformation.to_rdd] + 1 \n",
    "            else:\n",
    "                AnalysisHub.rdd_num_of_usage[transformation.to_rdd] = 1\n",
    "        for value in FactHub.job_last_rdd.values():\n",
    "            if value in AnalysisHub.rdd_num_of_usage.keys():  \n",
    "                AnalysisHub.rdd_num_of_usage[value] = AnalysisHub.rdd_num_of_usage[value] + 1\n",
    "            else:\n",
    "                AnalysisHub.rdd_num_of_usage[value] = 1\n",
    "        if not FactHub.shuffled_rdds_id:\n",
    "            for i, rdd in enumerate(FactHub.rdds_lst):\n",
    "                if \"ShuffledRDD\" in rdd.name or \"ShuffledRowRDD\" in rdd.name:\n",
    "                    FactHub.shuffled_rdds_id.append(rdd.id)\n",
    "        # Remove repeated elements from the list FactHub.shuffled_rdds_id\n",
    "        FactHub.shuffled_rdds_id = list(set(FactHub.shuffled_rdds_id))\n",
    "        for rdd_id in FactHub.shuffled_rdds_id:\n",
    "            rdd_shuffled_time = 0\n",
    "            for stage in FactHub.rddID_in_stage[rdd_id]:\n",
    "                rdd_shuffled_time = rdd_shuffled_time + FactHub.stage_shuffle_time_dict[stage]\n",
    "            FactHub.operator_timestamp[rdd_id] = rdd_shuffled_time\n",
    "        '''\n",
    "        # To recorrect caching plan list rdd's unpersist stage and job status\n",
    "        cached_rdds = set()\n",
    "        for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "            cached_rdds.add(caching_plan_item.rdd_id - 1)\n",
    "        print(\"cached_rdds\")\n",
    "        print(cached_rdds)\n",
    "        print(\"transformation_to_from\")\n",
    "        print(AnalysisHub.transformation_to_from)\n",
    "        print(\"FactHub.rddID_in_stage\")\n",
    "        print(FactHub.rddID_in_stage)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        for cached_rdd in cached_rdds:\n",
    "            for transformation in AnalysisHub.transformation_without_i:\n",
    "                if transformation.to_rdd == cached_rdd:\n",
    "                    while len(AnalysisHub.transformation_to_from[transformation.to_rdd]) != 0\n",
    "                        previous_transformation.to_rdd = transformation.to_rdd\n",
    "                        transformation.to_rdd = AnalysisHub.transformation_to_from[transformation.to_rdd][-1]\n",
    "                        if transformation.to_rdd in cached_rdds:\n",
    "                            occurance = 0\n",
    "                            for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "                                if caching_plan_item.rdd_id == previous_transformation.to_rdd:\n",
    "                                    occurance += 1\n",
    "                                if caching_plan_item.rdd_id == previous_transformation.to_rdd and occurance == 1:\n",
    "        '''\n",
    "                                \n",
    "    def cache_rdds_handling():\n",
    "        #To store cached rdds seperately in AnalysisHub\n",
    "        for rdd in FactHub.rdds_lst_refactored:\n",
    "            if rdd.is_cached:\n",
    "                AnalysisHub.cached_rdds_lst.append(FactHub.rdds_lst_index_dict[rdd.id])\n",
    "        AnalysisHub.cached_rdds_lst = list(dict.fromkeys(AnalysisHub.cached_rdds_lst))\n",
    "        print(\"AnalysisHub.cached_rdds_lst\")\n",
    "        print(AnalysisHub.cached_rdds_lst)\n",
    "        \n",
    "    def visualize_property_DAG():\n",
    "        dot = graphviz.Digraph(strict=True, comment='Spark-Application-Graph', format = config['Output']['selected_format'])\n",
    "        dot.attr('node', shape=config['Drawing']['rdd_shape'], label='this is graph')\n",
    "        dot.node_attr={'shape': 'plaintext'}\n",
    "        dot.edge_attr.update(arrowhead='normal', arrowsize='1')\n",
    "        dag_rdds_set = set()\n",
    "        prev_action_name = \"\"\n",
    "        iterations_count = int(config['Drawing']['max_iterations_count']) \n",
    "        for job_id, job in sorted(FactHub.job_last_rdd_dect.items()):\n",
    "            action_name = job[1]\n",
    "            draw_iteration_indicator = False\n",
    "            if action_name == prev_action_name:\n",
    "                if iterations_count == 0:\n",
    "                    continue\n",
    "                iterations_count-=1\n",
    "            else:\n",
    "                iterations_count = int(config['Drawing']['max_iterations_count']) \n",
    "            for rdd in FactHub.rdds_lst_refactored:\n",
    "                if rdd.job_id == job_id and rdd.id not in dag_rdds_set:\n",
    "                    dag_rdds_set.add(rdd.id)\n",
    "                    node_label = \"\\n\"\n",
    "                    if config['Drawing']['show_action_id'] == \"true\":\n",
    "                        renumbered_rdd_id = FactHub.rdds_lst_index_dict[rdd.id]\n",
    "                        node_label = \"[\" + str(renumbered_rdd_id) + \"] \"\n",
    "                        #node_label = \"[\" + str(rdd.id) + \"] \"\n",
    "                    if config['Drawing']['show_rdd_name'] == \"true\":\n",
    "                        node_label = node_label + rdd.name[:int(config['Drawing']['rdd_name_max_number_of_chars'])]\n",
    "                    if config['Drawing']['show_rdd_size'] == \"true\":\n",
    "                        if rdd.id in FactHub.rddID_size:\n",
    "                            #print(\"rdd id inside rddid_size\", str(rdd.id))\n",
    "                            size_in_mb = FactHub.rddID_size[rdd.id] / 1000000\n",
    "                            rounded_size = round(size_in_mb,3)\n",
    "                            if rounded_size >= 1024:\n",
    "                                rounded_size = rounded_size / 1024\n",
    "                                rounded_size = round(rounded_size,3)\n",
    "                                node_label = node_label + \"\\nsize: \" + str(rounded_size) + \" gb\"\n",
    "                            else:\n",
    "                                node_label = node_label + \"\\nsize: \" + str(rounded_size) + \" mb\"\n",
    "                        elif rdd.id in FactHub.root_rdd_size:\n",
    "                            #print(\"rdd id inside root_rdd_size\", str(rdd.id))\n",
    "                            size_in_mb = FactHub.root_rdd_size[rdd.id] / 1000000\n",
    "                            rounded_size = round(size_in_mb,3)\n",
    "                            if rounded_size >= 1024:\n",
    "                                rounded_size = rounded_size / 1024\n",
    "                                rounded_size = round(rounded_size,3)\n",
    "                                node_label = node_label + \"\\nsize: \" + str(rounded_size) + \" gb\"\n",
    "                            else:\n",
    "                                node_label = node_label + \"\\nsize: \" + str(rounded_size) + \" mb\"\n",
    "                        elif rdd.id in FactHub.last_rdd_size:\n",
    "                            #print(\"rdd id inside last_rdd_size\", str(rdd.id))\n",
    "                            size_in_mb = FactHub.last_rdd_size[rdd.id] / 1000000\n",
    "                            rounded_size = round(size_in_mb,3)\n",
    "                            if rounded_size >= 1024:\n",
    "                                rounded_size = rounded_size / 1024\n",
    "                                rounded_size = round(rounded_size,3)\n",
    "                                node_label = node_label + \"\\nsize: \" + str(rounded_size) + \" gb\"\n",
    "                            else:\n",
    "                                node_label = node_label + \"\\nsize: \" + str(rounded_size) + \" mb\"\n",
    "\n",
    "                    if config['Drawing']['show_rdd_computation_time'] == \"true\":\n",
    "                        reach_time = 0\n",
    "                        curr_rdd = rdd.id\n",
    "                        while 1: #loop runs until it reaches root parent node or cached node while traversing a RDD's parents\n",
    "                            if curr_rdd == 0 or curr_rdd not in AnalysisHub.transformation_from_to.keys():\n",
    "                                break\n",
    "                            prev_rdd = AnalysisHub.transformation_from_to[curr_rdd]\n",
    "                            if curr_rdd in FactHub.operator_timestamp.keys():\n",
    "                                reach_time = reach_time + FactHub.operator_timestamp[curr_rdd]\n",
    "                            prev_rdd_index = FactHub.rdds_lst_index_dict[prev_rdd]\n",
    "                            if prev_rdd_index in AnalysisHub.cached_rdds_lst or prev_rdd == 0:\n",
    "                                break\n",
    "                            curr_rdd = prev_rdd\n",
    "                            \n",
    "                        AnalysisHub.rdds_computation_time[rdd.id] = reach_time\n",
    "                        if reach_time >= 1000: #1000 is milliseconds ~1sec\n",
    "                            reach_time = reach_time / 1000\n",
    "                            reach_time = round(reach_time,1)\n",
    "                            if reach_time >= 60:\n",
    "                                reach_time = reach_time / 60\n",
    "                                reach_time = round(reach_time,1)\n",
    "                                if reach_time >= 60:\n",
    "                                    reach_time = reach_time / 60\n",
    "                                    reach_time = round(reach_time,1)\n",
    "                                    node_label = node_label + \"\\nComputation time ≈ \" + str(reach_time) + \" hr\"\n",
    "                                else:\n",
    "                                    node_label = node_label + \"\\nComputation time ≈ \" + str(reach_time) + \" min\"\n",
    "                            else:\n",
    "                                node_label = node_label + \"\\nComputation time ≈ \" + str(reach_time) + \" s\"\n",
    "                        else:\n",
    "                            node_label = node_label + \"\\nComputation time ≈ \" + str(reach_time) + \" ms\"\n",
    "                    if config['Caching_Anomalies']['show_number_of_rdd_usage'] == \"true\":\n",
    "                        node_label = node_label + \"\\nused: \" + str(AnalysisHub.rdd_num_of_usage[rdd.id])\n",
    "                    if config['Caching_Anomalies']['show_number_of_rdd_computations'] == \"true\":\n",
    "                        node_label = node_label + \"\\ncomputed: \" + str(AnalysisHub.rdd_num_of_computations[rdd.id])\n",
    "                    if  config['Caching_Anomalies']['highlight_unneeded_cached_rdds'] == \"true\" and AnalysisHub.anomalies_dict.get(rdd.id, \"\") == \"unneeded cache\":\n",
    "                        dot.node(str(rdd.id), penwidth = '3', fillcolor = config['Drawing']['cached_rdd_bg_color'], color = 'red', shape = config['Drawing']['anomaly_shape'], style = 'filled', label = node_label)\n",
    "                    elif config['Caching_Anomalies']['highlight_recomputed_rdds'] == \"true\" and AnalysisHub.anomalies_dict.get(rdd.id, \"\") == \"recomputation\":\n",
    "                        dot.node(str(rdd.id), penwidth = '3', fillcolor = 'white', color = 'red', shape = config['Drawing']['anomaly_shape'], style = 'filled', label = node_label)\n",
    "                    else:\n",
    "                        dot.node(str(rdd.id), fillcolor = config['Drawing']['cached_rdd_bg_color'] if FactHub.rdds_lst_index_dict[rdd.id] in AnalysisHub.cached_rdds_lst else 'white', style = 'filled', label = node_label)\n",
    "                        \n",
    "            action_lable = \"\" \n",
    "            if config['Drawing']['show_action_id'] == \"true\":\n",
    "                action_lable = \"[\" + str(job_id) + \"]\"\n",
    "            if config['Drawing']['show_action_name'] == \"true\":\n",
    "                action_lable = action_lable + action_name[:int(config['Drawing']['action_name_max_number_of_chars'])]\n",
    "            if draw_iteration_indicator == True:    \n",
    "                draw_iteration_indicator = False\n",
    "                continue\n",
    "            dot.node(\"Action_\" + str(job_id), shape=config['Drawing']['action_shape'] if iterations_count != 0 else config['Drawing']['iterative_action_shape'], fillcolor = config['Drawing']['action_bg_collor'] if iterations_count != 0 else config['Drawing']['iterative_action_collor'], style = 'filled', label = action_lable)\n",
    "            dot.edge(str(job[0]), \"Action_\" + str(job_id), color = 'black', arrowhead = 'none', style = 'dashed')\n",
    "            prev_action_name = action_name\n",
    "        \n",
    "        for transformation in AnalysisHub.transformation_without_i:\n",
    "            if transformation.to_rdd in dag_rdds_set and transformation.from_rdd in dag_rdds_set:\n",
    "                dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), color = config['Drawing']['narrow_transformation_color'] if transformation.is_narrow else config['Drawing']['wide_transformation_color'])\n",
    "        \n",
    "        for transformation in AnalysisHub.transformation_without_i:\n",
    "            #print(str(transformation.to_rdd) + \" \" + str(transformation.from_rdd))\n",
    "            if transformation.to_rdd in dag_rdds_set and transformation.from_rdd in dag_rdds_set:\n",
    "                if transformation.from_rdd in FactHub.operator_timestamp:\n",
    "                    milliseconds = FactHub.operator_timestamp[transformation.from_rdd]\n",
    "                    if milliseconds < 1000:\n",
    "                        dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(milliseconds)) + \" ms\")\n",
    "                    else:\n",
    "                        hours = milliseconds // (1000 * 60 * 60)\n",
    "                        remaining_milliseconds = milliseconds % (1000 * 60 * 60)\n",
    "                        minutes = remaining_milliseconds // (1000 * 60)\n",
    "                        remaining_milliseconds %= (1000 * 60)\n",
    "                        seconds = remaining_milliseconds // 1000\n",
    "                        milliseconds %= 1000\n",
    "                        if hours != 0 or minutes != 0:\n",
    "                            if hours != 0:\n",
    "                                dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(hours)) + \" h \" + str(int(minutes)) + \" m \" + str(int(seconds)) + \" s \" + str(int(milliseconds)) + \" ms\")\n",
    "                            else:\n",
    "                                dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(minutes)) + \" m \" + str(int(seconds)) + \" s \" + str(int(milliseconds)) + \" ms\")\n",
    "                        else:\n",
    "                            dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(seconds)) + \" s \" + str(int(milliseconds)) + \" ms\")\n",
    "            if transformation.from_rdd in FactHub.shuffled_rdds_id:\n",
    "                #print(\"transformation.from_rdd:  \" + str(transformation.from_rdd))\n",
    "                rdd_shuffled_time = 0\n",
    "                for stage in FactHub.rddID_in_stage[transformation.from_rdd]:\n",
    "                    rdd_shuffled_time = rdd_shuffled_time + FactHub.stage_shuffle_time_dict[stage]\n",
    "                if rdd_shuffled_time < 1000:\n",
    "                    FactHub.operator_timestamp[transformation.from_rdd] = rdd_shuffled_time \n",
    "                    dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(rdd_shuffled_time) + \" ms\")\n",
    "                else:\n",
    "                    FactHub.operator_timestamp[transformation.from_rdd] = rdd_shuffled_time\n",
    "                    milliseconds = rdd_shuffled_time\n",
    "                    hours = milliseconds // (1000 * 60 * 60)\n",
    "                    remaining_milliseconds = milliseconds % (1000 * 60 * 60)\n",
    "                    minutes = remaining_milliseconds // (1000 * 60)\n",
    "                    remaining_milliseconds %= (1000 * 60)\n",
    "                    seconds = remaining_milliseconds // 1000\n",
    "                    milliseconds %= 1000\n",
    "                    if hours != 0 or minutes != 0:\n",
    "                        if hours != 0:\n",
    "                            dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(hours)) + \" h \" + str(int(minutes)) + \" m \" + str(int(seconds)) + \" s \" + str(int(milliseconds)) + \" ms\")\n",
    "                        else:\n",
    "                            dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(minutes)) + \" m \" + str(int(seconds)) + \" s \" + str(int(milliseconds)) + \" ms\")\n",
    "                    else:\n",
    "                        dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), label = \"  \" + str(int(seconds)) + \" s \" + str(int(milliseconds)) + \" ms\")\n",
    "                                    \n",
    "        caching_plan_label = \"\\nRecommended Schedule:\\n\"\n",
    "        for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "            if caching_plan_item.is_cache_item:\n",
    "                caching_plan_label += \"\\nCache \"\n",
    "            else:\n",
    "                caching_plan_label += \"\\nUnpersist \"\n",
    "            #cache_rdd variable used below will store the correct rdd.id of the instrumented rdd's map partition's id after refactoring & renumbering the rdds list\n",
    "            cache_rdd = 0\n",
    "            if caching_plan_item.rdd_id-1 in FactHub.rdds_lst_index_dict.keys():\n",
    "                cache_rdd = FactHub.rdds_lst_index_dict[caching_plan_item.rdd_id-1]#caching_plan_label += \"RDD[\" + str(caching_plan_item.rdd_id - 1) + \"] \" + (\"at\" if caching_plan_item.is_cache_item else \"after\") + \" stage(\" + str(caching_plan_item.stage_id) + \") in job(\" + str(caching_plan_item.job_id) + \")\\n\"\n",
    "            caching_plan_label += \"RDD[\" + str(cache_rdd) + \"] \" + (\"at\" if caching_plan_item.is_cache_item else \"after\") + \" stage(\" + str(caching_plan_item.stage_id) + \") in job(\" + str(caching_plan_item.job_id) + \")\\n\"\n",
    "        caching_plan_label += \"\\n\"\n",
    "        if len(AnalysisHub.caching_plan_lst) > 0 and config['Caching_Anomalies']['show_caching_plan'] == \"true\":\n",
    "            dot.node(\"caching_plan\", shape = 'note', fillcolor = 'lightgray', style = 'filled', label = caching_plan_label)\n",
    "            \n",
    "        memory_footprint_label = \"\\nMemory Footprint:\\n\"\n",
    "        total_size = 0\n",
    "        temp = set()\n",
    "        for memory_footprint_item in AnalysisHub.memory_footprint_lst:\n",
    "            for val in memory_footprint_item[2]:\n",
    "                temp.add(val-1)\n",
    "        for val in temp:\n",
    "                if val in FactHub.rddID_size:\n",
    "                    size_in_mb = FactHub.rddID_size[val] / 1000000\n",
    "                    rounded_size = round(size_in_mb,3)\n",
    "                elif val in FactHub.root_rdd_size:\n",
    "                    size_in_mb = FactHub.root_rdd_size[val] / 1000000\n",
    "                    rounded_size = round(size_in_mb,3)\n",
    "                elif val in FactHub.last_rdd_size:\n",
    "                    size_in_mb = FactHub.last_rdd_size[val] / 1000000\n",
    "                    rounded_size = round(size_in_mb,3)\n",
    "                total_size = total_size + rounded_size\n",
    "        cached_rddsID = set()\n",
    "        for memory_footprint_item in AnalysisHub.memory_footprint_lst:\n",
    "            for val in memory_footprint_item[2]:\n",
    "                cached_rddsID.add(FactHub.rdds_lst_index_dict[val-1])\n",
    "        cached_rddsID = sorted(list(dict.fromkeys(cached_rddsID)))\n",
    "        print(\"cached_rddsID\")\n",
    "        print(cached_rddsID)\n",
    "        memory_footprint_label += \"\\n\"\n",
    "        if len(cached_rddsID) == 0:\n",
    "            memory_footprint_label += \"Free\"\n",
    "        else:\n",
    "            memory_footprint_label += str(cached_rddsID)\n",
    "        memory_footprint_label += \"\\n\"\n",
    "        memory_footprint_label += \"\\n\"\n",
    "        memory_footprint_label += \"Total size of cached RDDs: \" + str(total_size) + \" mb\"\n",
    "        memory_footprint_label += \"\\n\"\n",
    "        memory_footprint_label += \"\\n\"\n",
    "        \n",
    "        #FactHub.rddID_size[42] = 97000000\n",
    "        #FactHub.operator_timestamp[56] = 67\n",
    "        print(\"operator timestamp\")\n",
    "        print(FactHub.operator_timestamp)\n",
    "        print(\"rdd size\")\n",
    "        print(FactHub.rddID_size)\n",
    "        \n",
    "        #Time model scenarios\n",
    "        if len(cached_rddsID) > 0:\n",
    "            for rdd_id in cached_rddsID:\n",
    "                rdd_id = list(FactHub.rdds_lst_index_dict.keys())[list(FactHub.rdds_lst_index_dict.values()).index(rdd_id)]\n",
    "                if rdd_id in FactHub.operator_timestamp:\n",
    "                    #Time model scenario 1: If dont cache at all\n",
    "                    AnalysisHub.time_model_scenario1[rdd_id] = AnalysisHub.rdd_num_of_usage[rdd_id] * FactHub.operator_timestamp[rdd_id]\n",
    "                    #Time model scenario 2: If cache in memory\n",
    "                    if AnalysisHub.rdd_num_of_usage[rdd_id] == 1:\n",
    "                        #size and time information doesnt exist for some rdds due to missing instrumentation from log files\n",
    "                        time_taken_to_write = FactHub.rddID_size[rdd_id] / AnalysisHub.memory_write_capacity\n",
    "                        AnalysisHub.time_model_scenario2[rdd_id] = time_taken_to_write\n",
    "                    else:\n",
    "                        time_taken_to_write = FactHub.rddID_size[rdd_id] / AnalysisHub.memory_write_capacity\n",
    "                        time_taken_to_read = FactHub.rddID_size[rdd_id] / AnalysisHub.memory_read_capacity\n",
    "                        total_time_taken_to_read = (AnalysisHub.rdd_num_of_usage[rdd_id] - 1) * time_taken_to_read \n",
    "                        AnalysisHub.time_model_scenario2[rdd_id] = time_taken_to_write + total_time_taken_to_read\n",
    "                    #Time model scenario 3: If cache in disk\n",
    "                    if AnalysisHub.rdd_num_of_usage[rdd_id] == 1:\n",
    "                        #size and time information doesnt exist for some rdds due to missing instrumentation from log files\n",
    "                        time_taken_to_write = FactHub.rddID_size[rdd_id] / AnalysisHub.disk_write_capacity\n",
    "                        AnalysisHub.time_model_scenario3[rdd_id] = time_taken_to_write\n",
    "                    else:\n",
    "                        time_taken_to_write = FactHub.rddID_size[rdd_id] / AnalysisHub.disk_write_capacity\n",
    "                        time_taken_to_read = FactHub.rddID_size[rdd_id] / AnalysisHub.disk_read_capacity\n",
    "                        total_time_taken_to_read = (AnalysisHub.rdd_num_of_usage[rdd_id] - 1) * time_taken_to_read \n",
    "                        AnalysisHub.time_model_scenario3[rdd_id] = time_taken_to_write + total_time_taken_to_read \n",
    "        \n",
    "        print(\"AnalysisHub.time_model_scenario1\")\n",
    "        print(AnalysisHub.time_model_scenario1)\n",
    "        print(\"AnalysisHub.time_model_scenario2\")\n",
    "        print(AnalysisHub.time_model_scenario2)\n",
    "        print(\"AnalysisHub.time_model_scenario3\")\n",
    "        print(AnalysisHub.time_model_scenario3)\n",
    "        '''\n",
    "        #debugging for log file ends with 821\n",
    "        print(\"FactHub.rddID_in_stage\")\n",
    "        print(FactHub.rddID_in_stage[54])\n",
    "        print(FactHub.rddID_in_stage[56])\n",
    "        print(\"stage 11 has what tasks\")\n",
    "        print(FactHub.stage_has_what_tasks[11])\n",
    "        '''\n",
    "        print(\"=============================\")\n",
    "        for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "            print(\"rdd_id: \" + str(caching_plan_item.rdd_id) + \" stage_id: \" + str(caching_plan_item.stage_id) +  \" job_id: \" + str(caching_plan_item.job_id) + \" cache_status: \" + str(caching_plan_item.is_cache_item))\n",
    "        print(\"=============================\")\n",
    "        # To change persist and unpersit order for an edge case-(unpersisting of RDD A happens immediatedly after persisting RDD B in same stage and job)\n",
    "        modified_analysishub_caching_plan_lst = sorted(AnalysisHub.caching_plan_lst)\n",
    "        for i, caching_plan_item in enumerate(modified_analysishub_caching_plan_lst):\n",
    "            if i < len(AnalysisHub.caching_plan_lst) - 1:\n",
    "                next_instance = modified_analysishub_caching_plan_lst[i+1]\n",
    "                curr_item_rdd_id = caching_plan_item.rdd_id\n",
    "                curr_item_stage = caching_plan_item.stage_id\n",
    "                curr_item_job = caching_plan_item.job_id\n",
    "                curr_item_cache = caching_plan_item.is_cache_item\n",
    "                next_item_rdd_id = next_instance.rdd_id\n",
    "                next_item_stage = next_instance.stage_id\n",
    "                next_item_job = next_instance.job_id\n",
    "                next_item_cache = next_instance.is_cache_item\n",
    "                print(\"curr rdd id: \" + str(caching_plan_item.rdd_id) + \" curr_item_stage: \" + str(curr_item_stage) + \" curr_item_job: \" + str(curr_item_job) + \" curr_item_cache: \" + str(curr_item_cache) + \" | \" + \" next_instance: \" + str(next_instance.rdd_id) + \" next_item_stage: \" + str(next_item_stage) + \" next_item_job: \" + str(next_item_job) + \" next_item_cache: \" + str(next_item_cache))\n",
    "                if curr_item_cache == True and next_item_cache == False and curr_item_stage == next_item_stage and curr_item_job == next_item_job and curr_item_rdd_id != next_item_rdd_id:\n",
    "                    instance_to_move = modified_analysishub_caching_plan_lst.pop(i+1)\n",
    "                    modified_analysishub_caching_plan_lst.insert(i, instance_to_move)\n",
    "        print(\"=============================\")\n",
    "        for caching_plan_item in modified_analysishub_caching_plan_lst:\n",
    "            print(\"rdd_id: \" + str(caching_plan_item.rdd_id) + \" stage_id: \" + str(caching_plan_item.stage_id) +  \" job_id: \" + str(caching_plan_item.job_id) + \" cache_status: \" + str(caching_plan_item.is_cache_item))\n",
    "        \n",
    "        \n",
    "        #for caching_plan_item in modified_analysishub_caching_plan_lst:\n",
    "        for caching_plan_item in modified_analysishub_caching_plan_lst:\n",
    "            #print(\"rdd_id: \" + str(caching_plan_item.rdd_id) + \" stage_id: \" + str(caching_plan_item.stage_id) +  \" job_id: \" + str(caching_plan_item.job_id) + \" persist_status: \" + str(caching_plan_item.persist_status))\n",
    "            if caching_plan_item.is_cache_item:\n",
    "                if AnalysisHub.time_model_scenario2[caching_plan_item.rdd_id-1] < AnalysisHub.time_model_scenario3[caching_plan_item.rdd_id-1]:\n",
    "                    persist_status = \"memory\"\n",
    "                elif AnalysisHub.time_model_scenario2[caching_plan_item.rdd_id-1] > AnalysisHub.time_model_scenario3[caching_plan_item.rdd_id-1]:\n",
    "                    persist_status = \"disk\"\n",
    "                AnalysisHub.rdd_cache_order.append(CachingPlanItemWithStorage(caching_plan_item.stage_id, caching_plan_item.job_id, caching_plan_item.rdd_id-1, caching_plan_item.is_cache_item, persist_status))\n",
    "            else:\n",
    "                persist_status = \"unpersist\"\n",
    "                AnalysisHub.rdd_cache_order.append(CachingPlanItemWithStorage(caching_plan_item.stage_id, caching_plan_item.job_id, caching_plan_item.rdd_id-1, caching_plan_item.is_cache_item, persist_status))\n",
    "        \n",
    "        #for caching_plan_item in AnalysisHub.rdd_cache_order:\n",
    "            #print(\"rdd_id: \" + str(caching_plan_item.rdd_id) + \" stage_id: \" + str(caching_plan_item.stage_id) +  \" job_id: \" + str(caching_plan_item.job_id) + \" persist_status: \" + str(caching_plan_item.persist_status))\n",
    "        \n",
    "        '''\n",
    "        print(\"AnalysisHub.rdd_cache_order\")        \n",
    "        for caching_plan_item in AnalysisHub.rdd_cache_order:\n",
    "            print(\"caching_plan_item.rdd_id: \" + str(caching_plan_item.rdd_id))\n",
    "            print(\"caching_plan_item.is_cache_item: \" + str(caching_plan_item.is_cache_item))\n",
    "            print(\"caching_plan_item.stage_id: \" + str(caching_plan_item.stage_id))\n",
    "            print(\"caching_plan_item.job_id: \" + str(caching_plan_item.job_id))\n",
    "            print(\"caching_plan_item.persist_status: \" + str(caching_plan_item.persist_status))\n",
    "        '''\n",
    "        \n",
    "        def check_memory_management_list(rdd_id, stage_id, job_id):\n",
    "            nonlocal memory_footprint_label\n",
    "            low_priority_rdd = 0\n",
    "            lowest_priority_rdd = 0\n",
    "            for rdd in AnalysisHub.memory_management_list:\n",
    "                if AnalysisHub.rdd_num_of_usage[rdd_id] > AnalysisHub.rdd_num_of_usage[rdd]:\n",
    "                    low_priority_rdd = rdd\n",
    "                if lowest_priority_rdd == 0:\n",
    "                    lowest_priority_rdd = low_priority_rdd\n",
    "            if lowest_priority_rdd != 0:\n",
    "                for caching_plan_item in AnalysisHub.rdd_cache_order:\n",
    "                    if caching_plan_item.rdd_id == lowest_priority_rdd:\n",
    "                            AnalysisHub.memory_size = AnalysisHub.memory_size + FactHub.rddID_size[low_priority_rdd]\n",
    "                            memory_footprint_label += \"Unpersist RDD[\" + str(FactHub.rdds_lst_index_dict[caching_plan_item.rdd_id]) + \"] from \"+ \"memory at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                            memory_footprint_label += \"persist RDD[\" + str(FactHub.rdds_lst_index_dict[caching_plan_item.rdd_id]) + \"] from \"+ \"memory at disk \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                            AnalysisHub.persist_status_recorder[caching_plan_item.rdd_id] = \"disk\"\n",
    "                            AnalysisHub.memory_management_list.remove(caching_plan_item.rdd_id)\n",
    "                            AnalysisHub.disk_management_list.append(caching_plan_item.rdd_id)\n",
    "                memory_footprint_label += \"Persist RDD[\" + str(FactHub.rdds_lst_index_dict[rdd_id]) + \"] in \"+ \"memory at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                AnalysisHub.persist_status_recorder[rdd_id] = \"memory\"\n",
    "                AnalysisHub.memory_management_list.append(rdd_id)\n",
    "            #Persist directly to disk if no low priority rdd found in memory\n",
    "            if lowest_priority_rdd == 0:\n",
    "                AnalysisHub.disk_size = AnalysisHub.disk_size - FactHub.rddID_size[rdd_id]\n",
    "                memory_footprint_label += \"Persist RDD[\" + str(FactHub.rdds_lst_index_dict[rdd_id]) + \"] in \"+ \"disk at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"    \n",
    "                AnalysisHub.persist_status_recorder[rdd_id] = \"disk\"\n",
    "                AnalysisHub.disk_management_list.append(rdd_id)\n",
    "            \n",
    "        def check_disk_management_list(stage_id, job_id):\n",
    "            nonlocal memory_footprint_label\n",
    "            if len(AnalysisHub.disk_management_list) != 0:\n",
    "                high_priority_rdd = AnalysisHub.disk_management_list[0]\n",
    "                highest_priority_rdd = AnalysisHub.disk_management_list[0]\n",
    "                if len(AnalysisHub.disk_management_list) > 1:\n",
    "                    for rdd in AnalysisHub.disk_management_list[1:]:\n",
    "                        if AnalysisHub.rdd_num_of_usage[rdd] > AnalysisHub.rdd_num_of_usage[high_priority_rdd]:\n",
    "                            high_priority_rdd = rdd\n",
    "                        if AnalysisHub.rdd_num_of_usage[high_priority_rdd] > AnalysisHub.rdd_num_of_usage[highest_priority_rdd]:\n",
    "                            highest_priority_rdd = high_priority_rdd\n",
    "                if FactHub.rddID_size[highest_priority_rdd] < AnalysisHub.memory_size:\n",
    "                        AnalysisHub.disk_size = AnalysisHub.disk_size + FactHub.rddID_size[highest_priority_rdd]\n",
    "                        memory_footprint_label += \"Unpersist RDD[\" + str(FactHub.rdds_lst_index_dict[highest_priority_rdd]) + \"] from \"+ \"disk at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                        AnalysisHub.disk_management_list.remove(highest_priority_rdd)\n",
    "                        AnalysisHub.memory_size = AnalysisHub.memory_size - FactHub.rddID_size[highest_priority_rdd]\n",
    "                        memory_footprint_label += \"Persist RDD[\" + str(FactHub.rdds_lst_index_dict[highest_priority_rdd]) + \"] in \"+ \"memory at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                        AnalysisHub.persist_status_recorder[highest_priority_rdd] = \"memory\"\n",
    "                        AnalysisHub.memory_management_list.append(highest_priority_rdd)\n",
    "                        \n",
    "        for caching_plan_item in AnalysisHub.rdd_cache_order:\n",
    "            rdd_id = caching_plan_item.rdd_id\n",
    "            stage_id = caching_plan_item.stage_id\n",
    "            job_id = caching_plan_item.job_id\n",
    "            persist_status = caching_plan_item.persist_status\n",
    "            #print(\"rdd_id: \" + str(rdd_id) + \" stage_id: \" + str(stage_id) +  \" job_id: \" + str(job_id) + \" persist_status: \" + str(persist_status))\n",
    "            if caching_plan_item.is_cache_item:\n",
    "                if persist_status == \"memory\":\n",
    "                    if FactHub.rddID_size[rdd_id] < AnalysisHub.memory_size:\n",
    "                        AnalysisHub.memory_size = AnalysisHub.memory_size - FactHub.rddID_size[rdd_id] \n",
    "                        memory_footprint_label += \"Persist RDD[\" + str(FactHub.rdds_lst_index_dict[rdd_id]) + \"] in \"+ \"memory at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                        AnalysisHub.persist_status_recorder[rdd_id] = \"memory\"\n",
    "                        AnalysisHub.memory_management_list.append(rdd_id)\n",
    "                    else:\n",
    "                        check_memory_management_list(rdd_id, stage_id, job_id)\n",
    "                #Test case todo - what if disk has limited capacity\n",
    "                elif persist_status == \"disk\":\n",
    "                    AnalysisHub.disk_size = AnalysisHub.disk_size - FactHub.rddID_size[rdd_id]\n",
    "                    memory_footprint_label += \"Persist RDD[\" + str(FactHub.rdds_lst_index_dict[rdd_id]) + \"] in \"+ \"disk at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                    AnalysisHub.persist_status_recorder[rdd_id] = \"disk\"\n",
    "                    AnalysisHub.disk_management_list.append(rdd_id)\n",
    "            else:\n",
    "                #print(\"AnalysisHub.persist_status_recorder[rdd_id]: \" + str(rdd_id) + \" \" + str(AnalysisHub.persist_status_recorder[rdd_id]))\n",
    "                if AnalysisHub.persist_status_recorder[rdd_id] == \"memory\":\n",
    "                    AnalysisHub.memory_size = AnalysisHub.memory_size + FactHub.rddID_size[rdd_id] \n",
    "                    memory_footprint_label += \"UnPersist RDD[\" + str(FactHub.rdds_lst_index_dict[rdd_id]) + \"] from \"+ \"memory at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                    del AnalysisHub.persist_status_recorder[rdd_id]\n",
    "                    AnalysisHub.memory_management_list.remove(rdd_id)\n",
    "                    check_disk_management_list(stage_id, job_id)\n",
    "                elif AnalysisHub.persist_status_recorder[rdd_id] == \"disk\":\n",
    "                    AnalysisHub.memory_size = AnalysisHub.disk_size + FactHub.rddID_size[rdd_id] \n",
    "                    memory_footprint_label += \"UnPersist RDD[\" + str(FactHub.rdds_lst_index_dict[rdd_id]) + \"] from \"+ \"disk at stage \" + str(stage_id) + \" job \" + str(job_id) + \"\\n\"\n",
    "                    del AnalysisHub.persist_status_recorder[rdd_id]\n",
    "                    AnalysisHub.disk_management_list.remove(rdd_id)\n",
    "            '''\n",
    "            print(\"AnalysisHub.memory_size\")\n",
    "            print(AnalysisHub.memory_size)\n",
    "            print(\"AnalysisHub.disk_size\")\n",
    "            print(AnalysisHub.disk_size)\n",
    "            print(\"AnalysisHub.memory_management_list\")\n",
    "            print(AnalysisHub.memory_management_list)\n",
    "            print(\"AnalysisHub.disk_management_list\")\n",
    "            print(AnalysisHub.disk_management_list)\n",
    "            print(\"AnalysisHub.persist_status_recorder\")\n",
    "            print(AnalysisHub.persist_status_recorder)\n",
    "            '''\n",
    "        '''\n",
    "        print(\"without sorted\")\n",
    "        for i, caching_plan_item in enumerate(AnalysisHub.caching_plan_lst):\n",
    "            print(\"rdd id: \" + str(caching_plan_item.rdd_id) + \" stage: \" + str(caching_plan_item.stage_id) + \" job: \" + str(caching_plan_item.job_id) + \" cache: \" + str(caching_plan_item.is_cache_item))\n",
    "        print(\"with sorted\")\n",
    "        for i, caching_plan_item in enumerate(sorted(AnalysisHub.caching_plan_lst)):\n",
    "            print(\"rdd id: \" + str(caching_plan_item.rdd_id) + \" stage: \" + str(caching_plan_item.stage_id) + \" job: \" + str(caching_plan_item.job_id) + \" cache: \" + str(caching_plan_item.is_cache_item))\n",
    "        '''\n",
    "             \n",
    "        if len(AnalysisHub.caching_plan_lst) > 0 and config['Caching_Anomalies']['show_memory_footprint'] == \"true\":\n",
    "            dot.node(\"memory_footprint\", shape = 'note', fillcolor = 'lightgray', style = 'filled', label = memory_footprint_label)\n",
    "        dot.attr(labelloc=\"t\")\n",
    "        dot.attr(label=FactHub.app_name)\n",
    "        dot.attr(fontsize='40')\n",
    "        spark_dataflow_visualizer_output_path = Utility.get_absolute_path(config['Paths']['output_path'])\n",
    "        output_file_name = re.sub('[^a-zA-Z0-9]+', '', FactHub.app_name)\n",
    "        dot.render(spark_dataflow_visualizer_output_path + '/' + output_file_name, view=config['Output']['view_after_render'] == 'true')\n",
    "        \n",
    "        AnalysisHub.flush()\n",
    "        \n",
    "def load_facthub_data(file_name, log_file_path):\n",
    "    facthubs_folder = \"FactHubs\"\n",
    "    file_path = os.path.join(facthubs_folder, file_name+\".pickle\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        #Load the data from the file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            FactHub.app_name = data['app_name']\n",
    "            print(FactHub.app_name)\n",
    "            FactHub.job_info_dect = {int(k): v for k, v in data['job_info_dect'].items()}\n",
    "            FactHub.stage_info_dect = {int(k): v for k, v in data['stage_info_dect'].items()}\n",
    "            FactHub.stage_job_dect = {int(k): v for k, v in data['stage_job_dect'].items()}\n",
    "            FactHub.stage_name_dect = {int(k): v for k, v in data['stage_name_dect'].items()}\n",
    "            FactHub.submitted_stage_last_rdd_dect = {int(k): v for k, v in data['submitted_stage_last_rdd_dect'].items()}\n",
    "            FactHub.stage_no_of_tasks = {int(k): v for k, v in data['job_info_dect'].items()}\n",
    "            FactHub.stage_i_operator_dect = {int(k): v for k, v in data['stage_i_operator_dect'].items()}\n",
    "            FactHub.stage_i_operators_id = {int(k): v for k, v in data['stage_i_operators_id'].items()}\n",
    "            FactHub.submitted_stages = set(data['submitted_stages'])\n",
    "            FactHub.rdds_lst = [Rdd.from_json(rdd) for rdd in data['rdds_lst']]\n",
    "            FactHub.operator_partition_size = {int(k): v for k, v in data['operator_partition_size'].items()}\n",
    "            FactHub.rddID_in_stage = {int(k): v for k, v in data['rddID_in_stage'].items()}\n",
    "            FactHub.stage_operator_partition = {int(k): v for k, v in data['stage_operator_partition'].items()}\n",
    "            FactHub.stage_total = {int(k): v for k, v in data['stage_total'].items()}\n",
    "            FactHub.job_last_rdd = {int(k): v for k, v in data['job_last_rdd'].items()}\n",
    "            FactHub.job_last_rdd_dect = {int(k): v for k, v in data['job_last_rdd_dect'].items()}\n",
    "            FactHub.job_last_stage = {int(k): v for k, v in data['job_last_stage'].items()}\n",
    "            FactHub.rdd_id_stage_with_max_tasks = {int(k): v for k, v in data['rdd_id_stage_with_max_tasks'].items()}\n",
    "            FactHub.task_in_which_stage = {int(k): v for k, v in data['task_in_which_stage'].items()}\n",
    "            FactHub.stage_has_what_tasks ={int(k): v for k, v in data['stage_has_what_tasks'].items()}\n",
    "            FactHub.rdds_lst_index_dict = {int(k): v for k, v in data['rdds_lst_index_dict'].items()}\n",
    "            FactHub.taskid_launchtime = {int(k): v for k, v in data['taskid_launchtime'].items()}\n",
    "            FactHub.taskid_finishtime = {int(k): v for k, v in data['taskid_finishtime'].items()}\n",
    "            FactHub.taskid_last_processing_time = {int(k): v for k, v in data['taskid_finishtime'].items()}\n",
    "            FactHub.taskid_operator_dect ={int(k): v for k, v in data['taskid_operator_dect'].items()}\n",
    "            FactHub.root_rdd_size = {int(k): v for k, v in data['root_rdd_size'].items()}\n",
    "            FactHub.rddID_size = {int(k): v for k, v in data['rddID_size'].items()}\n",
    "            FactHub.last_rdd_size = {int(k): v for k, v in data['last_rdd_size'].items()}\n",
    "            FactHub.operator_timestamp = {int(k): v for k, v in data['operator_timestamp'].items()}\n",
    "            FactHub.rdds_lst_renumbered = [Rdd.from_json(rdd) for rdd in data['rdds_lst_renumbered']]\n",
    "            FactHub.rdds_lst_refactored = [Rdd.from_json(rdd) for rdd in data['rdds_lst_refactored']]\n",
    "            FactHub.rdds_lst_InstrumentedRdds = [Rdd.from_json(rdd) for rdd in data['rdds_lst_InstrumentedRdds']]\n",
    "            FactHub.rdds_lst_InstrumentedRdds_id = [int(x) for x in data['rdds_lst_InstrumentedRdds_id']]\n",
    "            FactHub.stage_shuffle_writetime_dict = {int(k): v for k, v in data['stage_shuffle_writetime_dict'].items()}\n",
    "            FactHub.stage_shuffle_readtime_dict = {int(k): v for k, v in data['stage_shuffle_readtime_dict'].items()}\n",
    "            FactHub.stage_shuffle_time_dict = {int(k): v for k, v in data['stage_shuffle_time_dict'].items()}\n",
    "            FactHub.shuffled_rdds_id = [int(x) for x in data['shuffled_rdds_id']]\n",
    "            \n",
    "    else:\n",
    "        print(FactHub.app_name)\n",
    "        SparkDataflowVisualizer.init()\n",
    "        SparkDataflowVisualizer.parse(log_file_path)\n",
    "        SparkDataflowVisualizer.rdds_lst_refactor()\n",
    "        # Create the FactHubs folder in the current directory if it doesn't exist\n",
    "        if not os.path.exists(facthubs_folder):\n",
    "            os.makedirs(facthubs_folder)\n",
    "        # Serialize the instance data and write it to a file\n",
    "        data = {\n",
    "        \"app_name\": FactHub.app_name,\n",
    "        \"job_info_dect\": FactHub.job_info_dect,\n",
    "        \"stage_info_dect\": FactHub.stage_info_dect,\n",
    "        \"stage_job_dect\": FactHub.stage_job_dect,\n",
    "        \"stage_name_dect\": FactHub.stage_name_dect,\n",
    "        \"submitted_stage_last_rdd_dect\": FactHub.submitted_stage_last_rdd_dect,\n",
    "        \"stage_no_of_tasks\": FactHub.stage_no_of_tasks,\n",
    "        \"stage_i_operator_dect\": dict(FactHub.stage_i_operator_dect),\n",
    "        \"stage_i_operators_id\": dict(FactHub.stage_i_operators_id),\n",
    "        \"submitted_stages\": list(FactHub.submitted_stages),\n",
    "        \"rdds_lst\": [rdd.to_json() for rdd in FactHub.rdds_lst],\n",
    "        \"operator_partition_size\": FactHub.operator_partition_size,\n",
    "        \"rddID_in_stage\": dict(FactHub.rddID_in_stage),\n",
    "        \"stage_operator_partition\": FactHub.stage_operator_partition,\n",
    "        \"stage_total\": FactHub.stage_total,\n",
    "        \"job_last_rdd\": FactHub.job_last_rdd,\n",
    "        \"job_last_rdd_dect\": FactHub.job_last_rdd_dect,\n",
    "        \"job_last_stage\": FactHub.job_last_stage,\n",
    "        \"rdd_id_stage_with_max_tasks\": FactHub.rdd_id_stage_with_max_tasks,\n",
    "        \"task_in_which_stage\": FactHub.task_in_which_stage,\n",
    "        \"stage_has_what_tasks\": dict(FactHub.stage_has_what_tasks),\n",
    "        \"rdds_lst_index_dict\": FactHub.rdds_lst_index_dict,\n",
    "        \"taskid_launchtime\": FactHub.taskid_launchtime,\n",
    "        \"taskid_finishtime\": FactHub.taskid_finishtime,\n",
    "        \"taskid_last_processing_time\": dict(FactHub.taskid_last_processing_time),\n",
    "        \"taskid_operator_dect\": dict(FactHub.taskid_operator_dect),\n",
    "        \"root_rdd_size\": FactHub.root_rdd_size,\n",
    "        \"rddID_size\": FactHub.rddID_size,\n",
    "        \"last_rdd_size\": FactHub.last_rdd_size,\n",
    "        \"operator_timestamp\": FactHub.operator_timestamp,\n",
    "        \"rdds_lst_renumbered\": [rdd.to_json() for rdd in FactHub.rdds_lst_renumbered],\n",
    "        \"rdds_lst_refactored\": [rdd.to_json() for rdd in FactHub.rdds_lst_refactored],\n",
    "        \"rdds_lst_InstrumentedRdds\": [rdd.to_json() for rdd in FactHub.rdds_lst_InstrumentedRdds],\n",
    "        \"rdds_lst_InstrumentedRdds_id\": [str(rdd) for rdd in FactHub.rdds_lst_InstrumentedRdds_id],\n",
    "        \"stage_shuffle_writetime_dict\": FactHub.stage_shuffle_writetime_dict,\n",
    "        \"stage_shuffle_readtime_dict\": FactHub.stage_shuffle_readtime_dict,\n",
    "        \"stage_shuffle_time_dict\": FactHub.stage_shuffle_time_dict,\n",
    "        \"shuffled_rdds_id\": [str(rdd) for rdd in FactHub.shuffled_rdds_id]\n",
    "        }\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "def load_file(file_name):\n",
    "    spark_dataflow_visualizer_input_path = Utility.get_absolute_path(config['Paths']['input_path'])\n",
    "    log_file_path = spark_dataflow_visualizer_input_path + '/' + file_name\n",
    "    load_facthub_data(file_name, log_file_path)\n",
    "\n",
    "def draw_DAG():\n",
    "    SparkDataflowVisualizer.analyze() \n",
    "    SparkDataflowVisualizer.rdds_lst_refactor()\n",
    "    SparkDataflowVisualizer.cache_rdds_handling()\n",
    "    SparkDataflowVisualizer.visualize_property_DAG()\n",
    "    #AnalysisHub.memory_footprint_lst = []\n",
    "    #AnalysisHub.cached_rdds_set.clear()\n",
    "    #AnalysisHub.non_cached_rdds_set.clear()\n",
    "    #AnalysisHub.cached_rdds_lst = []\n",
    "    \n",
    "def cache(rdd_id):\n",
    "    print(\"FactHub.rdds_lst_index_dict\")\n",
    "    print(FactHub.rdds_lst_index_dict)\n",
    "    key = list(FactHub.rdds_lst_index_dict.keys())[list(FactHub.rdds_lst_index_dict.values()).index(rdd_id)]\n",
    "    AnalysisHub.cached_rdds_set.add(key+1)\n",
    "    AnalysisHub.non_cached_rdds_set.discard(key+1)\n",
    "    print(\"AnalysisHub.cached_rdds_set\")\n",
    "    print(AnalysisHub.cached_rdds_set)\n",
    "    print(\"AnalysisHub.non_cached_rdds_set\")\n",
    "    print(AnalysisHub.non_cached_rdds_set)\n",
    "    #draw_DAG()\n",
    "    \n",
    "def dont_cache(rdd_id):\n",
    "    key = list(FactHub.rdds_lst_index_dict.keys())[list(FactHub.rdds_lst_index_dict.values()).index(rdd_id)]\n",
    "    AnalysisHub.non_cached_rdds_set.add(key+1)\n",
    "    AnalysisHub.cached_rdds_set.discard(key+1)\n",
    "    print(\"AnalysisHub.cached_rdds_set\")\n",
    "    print(AnalysisHub.cached_rdds_set)\n",
    "    print(\"AnalysisHub.non_cached_rdds_set\")\n",
    "    print(AnalysisHub.non_cached_rdds_set)\n",
    "    #SparkDataflowVisualizer.analyze() \n",
    "    #SparkDataflowVisualizer.rdds_lst_refactor()\n",
    "    #SparkDataflowVisualizer.visualize_property_DAG()\n",
    "    \n",
    "def storage_levels(mem_size, mem_read_capacity, mem_write_capacity, disk_size, disk_read_capacity, disk_write_capacity):\n",
    "    AnalysisHub.memory_size = mem_size * 1000000 #Convert MB to Bytes\n",
    "    AnalysisHub.memory_read_capacity = mem_read_capacity * 1000000000 #Convert GB to Bytes\n",
    "    AnalysisHub.memory_write_capacity = mem_write_capacity * 1000000000 #Convert GB to Bytes\n",
    "    AnalysisHub.disk_size = disk_size * 1000000 #Convert MB to Bytes\n",
    "    AnalysisHub.disk_read_capacity = disk_read_capacity * 1000000 #Convert MB to Bytes\n",
    "    AnalysisHub.disk_write_capacity = disk_write_capacity * 1000000 #Convert MB to Bytes\n",
    "    print(\"storage levels: \")\n",
    "    print(AnalysisHub.memory_size)\n",
    "    print(AnalysisHub.memory_read_capacity)\n",
    "    print(AnalysisHub.memory_write_capacity)\n",
    "    print(AnalysisHub.disk_size)\n",
    "    print(AnalysisHub.disk_read_capacity)\n",
    "    print(AnalysisHub.disk_write_capacity)\n",
    "    SparkDataflowVisualizer.analyze() \n",
    "    SparkDataflowVisualizer.rdds_lst_refactor()\n",
    "    SparkDataflowVisualizer.cache_rdds_handling()\n",
    "    SparkDataflowVisualizer.visualize_property_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60f7821d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseNaiveBayes with Params(hdfs://localhost:9000/HiBench/Bayes/Input,0,-1,1.0)\n",
      "FactHub.rdds_lst_index_dict\n",
      "{0: 0, 1: 1, 3: 2, 5: 3, 7: 4, 9: 5, 10: 6, 12: 7, 14: 8, 16: 9, 18: 10, 20: 11, 22: 12, 24: 13, 26: 14, 38: 15, 40: 16, 42: 17, 44: 18, 46: 19, 48: 20, 50: 21, 52: 22, 54: 23, 56: 24, 58: 25, 60: 26}\n",
      "AnalysisHub.cached_rdds_set\n",
      "{43}\n",
      "AnalysisHub.non_cached_rdds_set\n",
      "set()\n",
      "storage levels: \n",
      "100000000\n",
      "2000000000\n",
      "2000000000\n",
      "500000000\n",
      "500000000\n",
      "400000000\n",
      "AnalysisHub.cached_rdds_lst\n",
      "[9, 11, 17]\n",
      "cached_rddsID\n",
      "[9, 11, 17]\n",
      "operator timestamp\n",
      "{1: 3773, 3: 1057, 5: 33006, 7: 242735, 12: 162, 16: 8835, 20: 67, 22: 127, 26: 251, 38: 37, 40: 116, 42: 1264, 44: 26, 46: 36.9, 48: 104, 24: 38, 52: 4, 58: 159, 10: 0, 9: 1.1, 14: 13, 18: 402, 50: 7, 60: 4}\n",
      "rdd size\n",
      "{1: 252293472, 3: 371400000, 5: 591700000, 7: 996500000, 12: 67960, 16: 96800000, 20: 97000000, 22: 3964, 26: 78216832, 38: 79343552, 40: 160617600, 42: 41504, 44: 41568, 46: 20800, 48: 19744, 24: 8113, 52: 18827008, 58: 234240, 0: 194200064, 10: 2862, 9: 11819, 14: 1922, 18: 1922, 50: 0, 60: 0}\n",
      "AnalysisHub.time_model_scenario1\n",
      "defaultdict(<class 'int'>, {16: 17670, 20: 134, 42: 1264})\n",
      "AnalysisHub.time_model_scenario2\n",
      "defaultdict(<class 'int'>, {16: 0.0968, 20: 0.097, 42: 2.0752e-05})\n",
      "AnalysisHub.time_model_scenario3\n",
      "defaultdict(<class 'int'>, {16: 0.4356, 20: 0.4365, 42: 0.00010376})\n",
      "=============================\n",
      "rdd_id: 17 stage_id: 6 job_id: 3 cache_status: True\n",
      "rdd_id: 21 stage_id: 7 job_id: 4 cache_status: True\n",
      "rdd_id: 17 stage_id: 7 job_id: 4 cache_status: False\n",
      "rdd_id: 43 stage_id: 9 job_id: 6 cache_status: True\n",
      "rdd_id: 43 stage_id: 9 job_id: 6 cache_status: False\n",
      "rdd_id: 21 stage_id: 11 job_id: 7 cache_status: False\n",
      "=============================\n",
      "curr rdd id: 17 curr_item_stage: 6 curr_item_job: 3 curr_item_cache: True |  next_instance: 21 next_item_stage: 7 next_item_job: 4 next_item_cache: True\n",
      "curr rdd id: 21 curr_item_stage: 7 curr_item_job: 4 curr_item_cache: True |  next_instance: 17 next_item_stage: 7 next_item_job: 4 next_item_cache: False\n",
      "curr rdd id: 21 curr_item_stage: 7 curr_item_job: 4 curr_item_cache: True |  next_instance: 43 next_item_stage: 9 next_item_job: 6 next_item_cache: True\n",
      "curr rdd id: 43 curr_item_stage: 9 curr_item_job: 6 curr_item_cache: True |  next_instance: 43 next_item_stage: 9 next_item_job: 6 next_item_cache: False\n",
      "curr rdd id: 43 curr_item_stage: 9 curr_item_job: 6 curr_item_cache: False |  next_instance: 21 next_item_stage: 11 next_item_job: 7 next_item_cache: False\n",
      "=============================\n",
      "rdd_id: 17 stage_id: 6 job_id: 3 cache_status: True\n",
      "rdd_id: 17 stage_id: 7 job_id: 4 cache_status: False\n",
      "rdd_id: 21 stage_id: 7 job_id: 4 cache_status: True\n",
      "rdd_id: 43 stage_id: 9 job_id: 6 cache_status: True\n",
      "rdd_id: 43 stage_id: 9 job_id: 6 cache_status: False\n",
      "rdd_id: 21 stage_id: 11 job_id: 7 cache_status: False\n"
     ]
    }
   ],
   "source": [
    "#load_file('application_1676451203570_1317_1.bin') # ---> linear regression\n",
    "#load_file('application_1676451203570_2310_1')\n",
    "#load_file('local-1671802911751')\n",
    "#load_file('local-1659083213412')\n",
    "load_file('local-1671802703821')\n",
    "#load_file('application_1674642543712_0001.json')\n",
    "#load_file('application_1669826994084_2167_1')\n",
    "#load_file('application_1669826994084_2164_1')\n",
    "#load_file('application_1669826994084_2172_1')\n",
    "#load_file('application_1669826994084_2160_1')\n",
    "#load_file('application_1669826994084_2165_1')\n",
    "#load_file('application_1669826994084_2169_1')\n",
    "#load_file('application_1669826994084_2171_1')\n",
    "#load_file('application_1669826994084_2168_1')\n",
    "#load_file('application_1669826994084_2162_1')\n",
    "#load_file('application_1669826994084_2161_1')\n",
    "#load_file('application_1669826994084_2170_1')\n",
    "#load_file('application_1669826994084_2166_1')\n",
    "#load_file('local-1671802703821') \n",
    "#load_file('local-1671802911751')\n",
    "#load_file('application_1676451203570_0461_1.bin')\n",
    "config.read('config.ini') \n",
    "config['Drawing']['show_rdd_size'] = 'true' \n",
    "config['Drawing']['show_rdd_name'] = 'true' \n",
    "config['Drawing']['show_rdd_computation_time'] = 'true' \n",
    "config['Caching_Anomalies']['highlight_unneeded_cached_rdds'] = 'true'\n",
    "config['Caching_Anomalies']['highlight_recomputed_rdds'] = 'true'\n",
    "config['Caching_Anomalies']['show_number_of_rdd_usage'] = 'true'\n",
    "cache(17)\n",
    "#dont_cache(11)\n",
    "storage_levels(100,2,2,500,500,400)\n",
    "#draw_DAG()\n",
    "#no_rdds_cached_cost()\n",
    "#flush_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "32639513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toyexample\n",
      "AnalysisHub.memory_footprint_lst\n",
      "[(0, 0, {1}), (2, 2, {1, 4}), (2, 2, {4}), (4, 4, set())]\n"
     ]
    }
   ],
   "source": [
    "#toyexample\n",
    "load_file('local-1688386299499')\n",
    "#caching example with cache\n",
    "#load_file('local-1690222360874')\n",
    "#caching example without cache\n",
    "#load_file('local-1690222655824')\n",
    "#config['Drawing']['show_rdd_name'] = 'true'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5846a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mllib.BisectingKMeansExample\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1641567765635_0023')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a589d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseKMeans with Params(hdfs://localhost:9000/HiBench/Kmeans/Input/samples,10,5,MEMORY_ONLY,Random)\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1635092038229_0122')\n",
    "config.read('config.ini')\n",
    "config['Caching_Anomalies']['highlight_recomputed_rdds'] = 'true'\n",
    "config['Caching_Anomalies']['highlight_unneeded_cached_rdds'] = 'true'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca68dbd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create SparkSession\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkUIExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Enable Spark UI and retrieve the URL\u001b[39;00m\n\u001b[0;32m      7\u001b[0m spark_ui_url \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39muiWebUrl\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.3-bin-hadoop3.2\\python\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.3-bin-hadoop3.2\\python\\pyspark\\context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.3-bin-hadoop3.2\\python\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.3-bin-hadoop3.2\\python\\pyspark\\context.py:331\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 331\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.3-bin-hadoop3.2\\python\\pyspark\\java_gateway.py:105\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkUIExample\").getOrCreate()\n",
    "\n",
    "# Enable Spark UI and retrieve the URL\n",
    "spark_ui_url = spark.sparkContext.uiWebUrl\n",
    "\n",
    "# Perform your Spark operations\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n",
    "\n",
    "# Print the Spark UI URL\n",
    "print(\"Spark UI URL:\", spark_ui_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9345638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bf3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2e350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abe1c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allshortestpaths.py\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1641567765635_0161')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b21165",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(35)\n",
    "cache(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a521c411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc.py\n"
     ]
    }
   ],
   "source": [
    "load_file('local-1641586266617')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d50e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Tree with Params(2,30,32,20,0.1,hdfs://localhost:9000/HiBench/GBT/Input)\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1635092038229_0130')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d52dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS with Params(hdfs://localhost:9000/HiBench/ALS/Input,10,0.1,10,2,2,true)\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1635092038229_0126')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2086e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWeightGraphX\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1635092038229_0144')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "addb77b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with Params(100,1.0,0.01,hdfs://localhost:9000/HiBench/SVM/Input,MEMORY_ONLY)\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1635092038229_0140')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c7e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Drawing']['max_iterations_count'] = '5'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b66c913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Caching_Anomalies']['rdds_computation_tolerance_threshold'] = '3'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5195fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c4d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40e9bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40dd1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f404aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5d76f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionWithLBFGS\n"
     ]
    }
   ],
   "source": [
    "load_file('application_1635092038229_0124')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d7556e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Drawing']['max_iterations_count'] = '5'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988a0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Caching_Anomalies']['rdds_computation_tolerance_threshold'] = '2'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de7e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Caching_Anomalies']['rdds_computation_tolerance_threshold'] = '3'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82ba0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef8438ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalaSort\n",
      "ScalaTeraSort\n",
      "ScalaWordCount\n",
      "ScalaAggregation\n",
      "ScalaJoin\n",
      "ScalaScan\n",
      "ScalaPageRank\n",
      "BayesDataGen with Params(hdfs://localhost:9000/HiBench/Bayes/Input,hdfs://localhost:9000/HiBench/Bayes/Input.parquet,-1,false,100,100,10)\n",
      "NaiveBayesExample with Params(hdfs://localhost:9000/HiBench/Bayes/Input.parquet,1.0)\n",
      "DenseKMeans with Params(hdfs://localhost:9000/HiBench/Kmeans/Input/samples,10,5,MEMORY_ONLY,Random)\n",
      "LogisticRegressionWithLBFGS\n",
      "ALS with Params(hdfs://localhost:9000/HiBench/ALS/Input,10,0.1,10,2,2,true)\n",
      "PCAExample\n",
      "Gradient Boosted Tree with Params(2,30,32,20,0.1,hdfs://localhost:9000/HiBench/GBT/Input)\n",
      "RFC with Params(hdfs://localhost:9000/HiBench/RF/Input,100,2,auto,gini,4,32)\n",
      "SVD with Params(1000,800,true,1g,hdfs://localhost:9000/HiBench/SVD/Input)\n",
      "LinearRegressionWithElasticNet\n",
      "LDA Example with Params(hdfs://localhost:9000/HiBench/LDA/Input,hdfs://localhost:9000/HiBench/LDA/Output,10,10,online,1g)\n",
      "SVM with Params(100,1.0,0.01,hdfs://localhost:9000/HiBench/SVM/Input,MEMORY_ONLY)\n",
      "GaussianMixtureModel$\n",
      "NWeightGraphX\n",
      "ch07-graph\n",
      "SVMWithSGDExample\n",
      "LogisticRegressionWithLBFGSExample\n",
      "PCAOnSourceVectorExample\n",
      "DecisionTreeClassificationExample\n",
      "SVDExample\n",
      "CorrelationsExample\n",
      "mllib.BisectingKMeansExample\n",
      "PCAOnRowMatrixExample\n",
      "GradientBoostedTreesRegressionExample\n",
      "GaussianMixtureExample\n",
      "Word2VecExample\n",
      "TFIDFExample\n",
      "KMeansExample\n",
      "KernelDensityEstimationExample\n",
      "HypothesisTestingKolmogorovSmirnovTestExample\n",
      "StandardScalerExample\n",
      "AssociationRulesExample\n",
      "RankingMetricsExample\n",
      "NaiveBayesExample\n",
      "PrefixSpanExample\n",
      "DecisionTreeRegressionExample\n",
      "PMMLModelExportExample\n",
      "HypothesisTestingExample\n",
      "RandomRDDGeneration\n",
      "LBFGSExample\n",
      "StratifiedSamplingExample\n",
      "MulticlassMetricsExample\n",
      "IsotonicRegressionExample\n",
      "RandomForestRegressionExample\n",
      "SVMWithSGDExample\n",
      "NormalizerExample\n",
      "SummaryStatisticsExample\n",
      "ElementwiseProductExample\n",
      "LogisticRegressionWithLBFGSExample\n",
      "BinaryClassificationMetricsExample\n",
      "SimpleFPGrowth\n",
      "CollaborativeFilteringExample\n",
      "GradientBoostedTreesClassificationExample\n",
      "RandomForestClassificationExample\n",
      "ChiSqSelectorExample\n",
      "DecisionTreeClassificationExample\n",
      "BisectingKMeansExample\n",
      "GradientBoostedTreeRegressorExample\n",
      "ImputerExample\n",
      "OneVsRestExample\n",
      "GaussianMixtureExample$\n",
      "FPGrowthExample$\n",
      "Word2Vec example\n",
      "LDAExample$\n",
      "MinHashLSHExample\n",
      "PCAExample\n",
      "MaxAbsScalerExample\n",
      "KMeansExample$\n",
      "QuantileDiscretizerExample\n",
      "PipelineExample\n",
      "DeveloperApiExample\n",
      "GeneralizedLinearRegressionExample\n",
      "EstimatorTransformerParamExample\n",
      "IndexToStringExample\n",
      "StandardScalerExample\n",
      "NaiveBayesExample\n",
      "LinearRegressionWithElasticNetExample\n",
      "MultilayerPerceptronClassifierExample\n",
      "PrefixSpanExample$\n",
      "BucketedRandomProjectionLSHExample\n",
      "DecisionTreeRegressionExample\n",
      "UnivariateFeatureSelectorExample\n",
      "LogisticRegressionSummaryExample\n",
      "TfIdfExample\n",
      "CorrelationExample\n",
      "CountVectorizerExample\n",
      "RFormulaExample\n",
      "RandomForestClassifierExample\n",
      "FMClassifierExample\n",
      "ModelSelectionViaTrainValidationSplitExample\n",
      "ALSExample\n",
      "VarianceThresholdSelectorExample\n",
      "MinMaxScalerExample\n",
      "UnaryTransformerExample\n",
      "IsotonicRegressionExample$\n",
      "ChiSquareTestExample\n",
      "StringIndexerExample\n",
      "RobustScalerExample\n",
      "AFTSurvivalRegressionExample\n",
      "LinearSVCExample\n",
      "RandomForestRegressorExample\n",
      "FMRegressorExample\n",
      "SummarizerExample\n",
      "VectorIndexerExample\n",
      "ModelSelectionViaCrossValidationExample\n",
      "LogisticRegressionWithElasticNetExample\n",
      "OneHotEncoderExample\n",
      "ChiSqSelectorExample\n",
      "GradientBoostedTreeClassifierExample\n",
      "MultiLabelMetricsExample\n",
      "ConnectedComponentsExample$\n",
      "SSSPExample$\n",
      "TriangleCountingExample$\n",
      "GraphX Synth Benchmark (nverts = 100000, app = pagerank)\n",
      "AggregateMessagesExample$\n",
      "bfs.py\n",
      "allshortestpaths.py\n",
      "degree.py\n",
      "cc.py\n",
      "triangles.py\n",
      "scc.py\n",
      "lpa.py\n",
      "unionfind.py\n"
     ]
    }
   ],
   "source": [
    "config.read('config.ini')\n",
    "config['Output']['view_after_render'] = 'false'\n",
    "config['Drawing']['max_iterations_count'] = '5'\n",
    "\n",
    "file_list = os.listdir(config['Paths']['input_path'])\n",
    "for os_file_name in file_list:\n",
    "    load_file(os_file_name)\n",
    "    draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1a0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
